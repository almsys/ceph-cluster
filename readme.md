# Ceph Production-Ready: ÐŸÐ¾Ð»Ð½Ð¾Ðµ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ð¾ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸ÑŽ Ð¸ ÑÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ð¸

## Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ

1. [Ð’Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ](#Ð²Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ)
2. [ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¸ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ](#Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°-Ð¸-Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ)
3. [Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ](#Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ-Ðº-Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ)
4. [ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹](#Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ°-Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹)
5. [Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ceph](#ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°-ceph)
6. [ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ](#ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ-Ð¸-Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ)
7. [ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°](#Ð¼Ð°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ-ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°)
8. [ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð°Ð»ÐµÑ€Ñ‚Ð¸Ð½Ð³](#Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³-Ð¸-Ð°Ð»ÐµÑ€Ñ‚Ð¸Ð½Ð³)
9. [Ð­ÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ñ Ð¸ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ](#ÑÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ñ-Ð¸-Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ)
10. [Troubleshooting](#troubleshooting)


---

## Ð’Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ

### Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ceph?

Ceph - ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ð¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¼ ÐºÐ¾Ð´Ð¾Ð¼, Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÑŽÑ‰Ð°Ñ:
- **Object Storage** (S3/Swift-ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ð¹)
- **Block Storage** (RBD Ð´Ð»Ñ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸)
- **File Storage** (CephFS)

### Ð”Ð»Ñ ÐºÐ¾Ð³Ð¾ ÑÑ‚Ð¾ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾?

Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð¿Ñ€ÐµÐ´Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½ Ð´Ð»Ñ Ð¸Ð½Ð¶ÐµÐ½ÐµÑ€Ð¾Ð², Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°ÑŽÑ‰Ð¸Ñ… production-ÐºÐ»Ð°ÑÑ‚ÐµÑ€ Ceph Ð¸ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð¾Ð¿Ñ‹Ñ‚Ðµ ÑÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ð¸.

### ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿Ñ€Ð¸Ð½Ñ†Ð¸Ð¿Ñ‹ production-Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ

âš ï¸ **ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž**: 
- ÐÐ¸ÐºÐ¾Ð³Ð´Ð° Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€ Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸ 1
- Ð’ÑÐµÐ³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð´Ð²Ðµ ÑÐµÑ‚Ð¸ (public + cluster)
- Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾ Ð·Ð°Ð¼ÐµÐ½ÑÐ¹Ñ‚Ðµ Ð´Ð¸ÑÐºÐ¸ (Ð½Ðµ ÑÑ‚Ð°Ñ€ÑˆÐµ 3-4 Ð»ÐµÑ‚)
- Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð° Ð²ÑÐµÑ… ÑƒÐ·Ð»Ð°Ñ…
- Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð¾Ñ‚ÐºÐ°Ð·Ñ‹ Ð² Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¼ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ð¸

---

## ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð° Ð¸ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ

### ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ production-ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Ceph Cluster                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Node 1    â”‚   Node 2    â”‚      Node 3         â”‚
â”‚   (Rack 1)  â”‚   (Rack 2)  â”‚     (Rack 3)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MON         â”‚ MON         â”‚ MON                 â”‚
â”‚ MGR(active) â”‚ MGR(standby)â”‚ MGR(standby)        â”‚
â”‚ 10x OSD     â”‚ 10x OSD     â”‚ 10x OSD             â”‚
â”‚ 4TB each    â”‚ 4TB each    â”‚ 4TB each            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Public:  25Gbit/s  (ÐºÐ»Ð¸ÐµÐ½Ñ‚ÑÐºÐ¸Ð¹ Ñ‚Ñ€Ð°Ñ„Ð¸Ðº)          â”‚
â”‚ Cluster: 25Gbit/s  (Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ, recovery)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ð¢Ð¸Ð¿Ð¾Ð²Ñ‹Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸

| Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° | Ð£Ð·Ð»Ð¾Ð² | OSD/ÑƒÐ·ÐµÐ» | Ð•Ð¼ÐºÐ¾ÑÑ‚ÑŒ | RAM/ÑƒÐ·ÐµÐ» | Use Case |
|----------------|-------|----------|---------|----------|----------|
| ÐœÐ°Ð»Ñ‹Ð¹ | 3 | 10 | 120 TB | 96 GB | Dev/Test, Small prod |
| Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹ | 5-7 | 12 | 300+ TB | 128 GB | Production |
| Ð‘Ð¾Ð»ÑŒÑˆÐ¾Ð¹ | 10+ | 12-24 | 1+ PB | 256+ GB | Enterprise |

### Ð Ð°ÑÑ‡ÐµÑ‚ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²

#### Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ñ‹ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ:

**Ð•Ð¼ÐºÐ¾ÑÑ‚ÑŒ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸:**
```
ÐŸÐ¾Ð»ÐµÐ·Ð½Ð°Ñ ÐµÐ¼ÐºÐ¾ÑÑ‚ÑŒ = (Ð¡Ñ‹Ñ€Ð°Ñ ÐµÐ¼ÐºÐ¾ÑÑ‚ÑŒ Ã— ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ OSD) / Ð¤Ð°ÐºÑ‚Ð¾Ñ€ Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
ÐŸÑ€Ð¸Ð¼ÐµÑ€: (4 TB Ã— 30 OSD) / 3 = 40 TB Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾Ð¹ ÐµÐ¼ÐºÐ¾ÑÑ‚Ð¸
```

**ÐžÐ¿ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ:**
```
RAM = (ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ OSD Ã— 8 GB) + 16 GB (ÑÐ¸ÑÑ‚ÐµÐ¼Ð°)
ÐŸÑ€Ð¸Ð¼ÐµÑ€: (10 OSD Ã— 8 GB) + 16 GB = 96 GB RAM
```

**CPU:**
```
ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 1-2 ÑÐ´Ñ€Ð° Ð½Ð° OSD
Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ: 24+ ÑÐ´ÐµÑ€ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð¾Ð¹ (3.0+ GHz)
```

### Ð¢Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ñ Ð¾Ñ‚ÐºÐ°Ð·Ð¾ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚Ð¸

**CRUSH Map** Ð´Ð¾Ð»Ð¶Ð½Ð° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ñ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ð¸:

```
root default
    â”œâ”€â”€ datacenter DC1
    â”‚   â”œâ”€â”€ rack rack1
    â”‚   â”‚   â””â”€â”€ host node1
    â”‚   â”‚       â”œâ”€â”€ osd.0
    â”‚   â”‚       â””â”€â”€ osd.1
    â”‚   â”œâ”€â”€ rack rack2
    â”‚   â”‚   â””â”€â”€ host node2
    â”‚   â”‚       â”œâ”€â”€ osd.2
    â”‚   â”‚       â””â”€â”€ osd.3
    â”‚   â””â”€â”€ rack rack3
    â”‚       â””â”€â”€ host node3
    â”‚           â”œâ”€â”€ osd.4
    â”‚           â””â”€â”€ osd.5
```

**ÐŸÑ€Ð°Ð²Ð¸Ð»Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ñ:**
- Ð”Ð°Ð½Ð½Ñ‹Ðµ Ñ€ÐµÐ¿Ð»Ð¸Ñ†Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ðµ ÑÑ‚Ð¾Ð¹ÐºÐ¸ (rack)
- ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 ÑÑ‚Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ Ð¾Ñ‚ÐºÐ°Ð·Ð¾ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚Ð¸
- ÐŸÑ€Ð¸ Ð¾Ñ‚ÐºÐ°Ð·Ðµ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ð¾Ð¹ÐºÐ¸ ÐºÐ»Ð°ÑÑ‚ÐµÑ€ Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ñ€Ð°Ð±Ð¾Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ñ‹Ð¼

---

## Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ðº Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸ÑŽ

### ÐŸÑ€Ð¾Ñ†ÐµÑÑÐ¾Ñ€ (CPU)

âœ… **Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸:**
- **ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ´ÐµÑ€**: 16-24+ (Ð½Ðµ Ð¼ÐµÐ½ÐµÐµ 1-2 ÑÐ´ÐµÑ€ Ð½Ð° OSD)
- **Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð°**: 3.0+ GHz Ð½Ð° ÑÐ´Ñ€Ð¾ (Ñ‡ÐµÐ¼ Ð²Ñ‹ÑˆÐµ, Ñ‚ÐµÐ¼ Ð»ÑƒÑ‡ÑˆÐµ)
- **ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°**: x86_64 (Intel Xeon, AMD EPYC)

âš™ï¸ **ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸:**
```bash
# ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ÑÐ½ÐµÑ€Ð³Ð¾ÑÐ±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¸Ðµ (C-states)
# Ceph Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¾Ð´Ð½Ð¾Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
# Ð¡Ð¾Ð½ ÑÐ´ÐµÑ€ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

**ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ ÑÑ‚Ð¾ Ð²Ð°Ð¶Ð½Ð¾?**
- Ceph Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð¼Ð½Ð¾Ð³Ð¾ Ð¾Ð´Ð½Ð¾Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹
- Ð‘Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÐ¸ Ð² ÐºÐ¾Ð´Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… ÑÐ´ÐµÑ€
- Ð¯Ð´Ñ€Ð¾ Ð² Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð¼ ÑÐ½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð¼Ð¸Ð»Ð»Ð¸ÑÐµÐºÑƒÐ½Ð´Ñ‹ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸

### ÐžÐ¿ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ (RAM)

âœ… **Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð¾Ð±ÑŠÐµÐ¼Ñƒ:**

| Ð¢Ð¸Ð¿ OSD | RAM Ð½Ð° OSD | ÐŸÑ€Ð¸Ð¼ÐµÑ€ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ |
|---------|------------|---------------------|
| SSD | 8 GB | 10 OSD = 80 GB + 16 GB ÑÐ¸ÑÑ‚ÐµÐ¼Ð° = 96 GB |
| HDD | 5 GB | 10 OSD = 50 GB + 16 GB ÑÐ¸ÑÑ‚ÐµÐ¼Ð° = 66 GB |
| NVMe | 8-12 GB | Ð—Ð°Ð²Ð¸ÑÐ¸Ñ‚ Ð¾Ñ‚ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ |

âš ï¸ **Ð’ÐÐ–ÐÐž:**
- Ceph **Ð¾Ñ‡ÐµÐ½ÑŒ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ½ Ðº Ð¿Ð°Ð¼ÑÑ‚Ð¸**
- ÐžÐ´Ð¸Ð½ OSD Ð¼Ð¾Ð¶ÐµÑ‚ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð»ÑÑ‚ÑŒ 30-40 GB Ð¿Ñ€Ð¸ Ð¼Ð°ÑÑÐ¾Ð²Ð¾Ð¼ recovery
- ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ðº Ð¿Ð°Ð¼ÑÑ‚Ð¸ = OOM killer = ÐºÐ°ÑÐºÐ°Ð´Ð½Ñ‹Ðµ Ð¾Ñ‚ÐºÐ°Ð·Ñ‹

ðŸ›¡ï¸ **Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ OOM:**
```bash
# ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· systemd (Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾!)
systemctl edit ceph-osd@0.service

[Service]
MemoryMax=8G      # Ð¶ÐµÑÑ‚ÐºÐ¸Ð¹ Ð»Ð¸Ð¼Ð¸Ñ‚
MemoryHigh=7G     # soft limit, Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ Ð·Ð°Ð¼ÐµÐ´Ð»ÑÑ‚ÑŒ
CPUQuota=200%     # Ð½Ðµ Ð±Ð¾Ð»ÐµÐµ 2 ÑÐ´ÐµÑ€
```

### Ð”Ð¸ÑÐºÐ¸ (Storage)

#### âŒ Ð§ÐµÐ³Ð¾ Ð¸Ð·Ð±ÐµÐ³Ð°Ñ‚ÑŒ:

- **HDD Ð´Ð¸ÑÐºÐ¸** (ÐºÑ€Ð¾Ð¼Ðµ cold storage)
- **Consumer SSD** Ð±ÐµÐ· power-loss protection
- **Ð”Ð¸ÑÐºÐ¸ ÑÑ‚Ð°Ñ€ÑˆÐµ 3-4 Ð»ÐµÑ‚** (Ð²Ñ‹ÑÐ¾ÐºÐ¸Ð¹ Ñ€Ð¸ÑÐº Ð¾Ñ‚ÐºÐ°Ð·Ð°)
- **Ð¡Ð¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ HDD Ð¸ SSD Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ð¿ÑƒÐ»Ðµ**

#### âœ… Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ Ð´Ð¸ÑÐºÐ¸:

**Tier 1: Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ**
- Intel DC P4610/P5520 (NVMe)
- Samsung PM1733/PM9A3 (NVMe)
- Micron 7450 (NVMe)
- Ð¥Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸: 3+ DWPD, power-loss protection

**Tier 2: Ð‘Ð°Ð»Ð°Ð½Ñ Ñ†ÐµÐ½Ð°/Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ**
- Intel D3-S4610/S4510 (SATA SSD)
- Samsung PM883/PM893 (SATA SSD)
- Micron 5300/5400 (SATA SSD)
- Ð¥Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸: 1-3 DWPD, power-loss protection

**Tier 3: Ð•Ð¼ÐºÐ¾ÑÑ‚ÑŒ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ cold storage)**
- Enterprise HDD 7200 RPM (Seagate Exos, WD Ultrastar)
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ñ€ÐµÐ´ÐºÐ¾ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…

#### ðŸ”§ Ð’Ð°Ð¶Ð½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸:

1. **Power-Loss Protection** (ÐºÐ¾Ð½Ð´ÐµÐ½ÑÐ°Ñ‚Ð¾Ñ€Ñ‹)
   - ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð´Ð»Ñ Ceph Ð¸Ð·-Ð·Ð° Ñ‡Ð°ÑÑ‚Ñ‹Ñ… fsync Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹
   - Ð”Ð¸ÑÐºÐ¸ Ð±ÐµÐ· PLP Ð¼ÐµÐ´Ð»ÐµÐ½Ð½ÐµÐµ Ð² 3-5 Ñ€Ð°Ð·
   - Ð Ð¸ÑÐº Ð¿Ð¾Ð²Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¸ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ

2. **DWPD (Drive Writes Per Day)**
   - ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 1 DWPD Ð´Ð»Ñ production
   - ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ 3+ DWPD Ð´Ð»Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
   - Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð° Ñ€Ð°ÑÑ‡ÐµÑ‚Ð°: `DWPD = TBW / (ÐµÐ¼ÐºÐ¾ÑÑ‚ÑŒ Ð² TB Ã— 365 Ð´Ð½ÐµÐ¹ Ã— Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ)`

3. **Ð Ð°Ð·Ð¼ÐµÑ€ Ð´Ð¸ÑÐºÐ°**
   - ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾: 2-8 TB
   - Ð¡Ð»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ Ð´Ð¸ÑÐºÐ¸ (16+ TB) ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÑŽÑ‚ Ð²Ñ€ÐµÐ¼Ñ recovery
   - ÐœÐ½Ð¾Ð³Ð¾ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ñ… Ð´Ð¸ÑÐºÐ¾Ð² = Ð±Ð¾Ð»ÑŒÑˆÐµ OSD = Ð±Ð¾Ð»ÑŒÑˆÐµ RAM

#### ðŸ’¡ ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ NVMe:

```bash
# Ð”Ð»Ñ NVMe Ð¼Ð¾Ð¶Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ OSD Ð½Ð° Ð¾Ð´Ð½Ð¾Ð¼ Ð´Ð¸ÑÐºÐµ
# Ð­Ñ‚Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÑ‚ ÑƒÑ‚Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð·Ð° ÑÑ‡ÐµÑ‚ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼Ð°

# ÐŸÑ€Ð¸Ð¼ÐµÑ€: 1 NVMe Ð´Ð¸ÑÐº = 3 OSD
ceph-volume lvm create --data /dev/nvme0n1p1
ceph-volume lvm create --data /dev/nvme0n1p2
ceph-volume lvm create --data /dev/nvme0n1p3

# ÐŸÐ»ÑŽÑÑ‹:
# - Ð’Ñ‹ÑˆÐµ ÑƒÑ‚Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ð´Ð¸ÑÐºÐ¾Ð²
# - Ð›ÑƒÑ‡ÑˆÐ¸Ð¹ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼

# ÐœÐ¸Ð½ÑƒÑÑ‹:
# - Ð¡Ð»Ð¾Ð¶Ð½ÐµÐµ Ð² ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸
# - ÐŸÑ€Ð¸ Ð¾Ñ‚ÐºÐ°Ð·Ðµ Ð´Ð¸ÑÐºÐ° Ñ‚ÐµÑ€ÑÐµÑ‚ÑÑ 3 OSD
```

### Ð¡ÐµÑ‚ÑŒ (Network)

#### âš ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: Ð”Ð²Ðµ ÑÐµÑ‚Ð¸ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹!

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Public Network              â”‚
â”‚      (ÐºÐ»Ð¸ÐµÐ½Ñ‚ÑÐºÐ¸Ð¹ Ñ‚Ñ€Ð°Ñ„Ð¸Ðº)             â”‚
â”‚      25+ Gbit/s bonded               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Client   â”‚  Client   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cluster Network              â”‚
â”‚    (Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ, recovery)            â”‚
â”‚      25+ Gbit/s bonded               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“         â†“         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Node 1 â”‚ Node 2 â”‚ Node 3 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð´Ð²Ðµ ÑÐµÑ‚Ð¸?**
- Recovery/rebalance ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð°Ñ„Ð¸Ðº
- Ð‘ÐµÐ· Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÑŽÑ‚ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ðµ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸
- Cluster network Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð´ÐµÑˆÐµÐ²Ð»Ðµ (Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸)

#### âœ… Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸:

| Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð° | Public Network | Cluster Network | Bonding |
|----------------|----------------|-----------------|---------|
| ÐœÐ°Ð»Ñ‹Ð¹ (3-5 ÑƒÐ·Ð»Ð¾Ð²) | 25 Gbit/s | 25 Gbit/s | LACP |
| Ð¡Ñ€ÐµÐ´Ð½Ð¸Ð¹ (5-10 ÑƒÐ·Ð»Ð¾Ð²) | 50 Gbit/s | 50 Gbit/s | LACP |
| Ð‘Ð¾Ð»ÑŒÑˆÐ¾Ð¹ (10+ ÑƒÐ·Ð»Ð¾Ð²) | 100 Gbit/s | 100 Gbit/s | LACP |

âŒ **ÐÐµÐ¿Ñ€Ð¸ÐµÐ¼Ð»ÐµÐ¼Ð¾ Ð´Ð»Ñ production:**
- ÐžÐ´Ð½Ð° ÑÐµÑ‚ÑŒ (public == cluster)
- Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ Ð¼ÐµÐ½ÐµÐµ 10 Gbit/s
- ÐžÐ±Ñ‰Ð¸Ðµ ÐºÐ¾Ð¼Ð¼ÑƒÑ‚Ð°Ñ‚Ð¾Ñ€Ñ‹ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ ÑÐµÑ€Ð²Ð¸ÑÐ°Ð¼Ð¸ Ð±ÐµÐ· QoS

#### ðŸ”§ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° bonding (Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð´Ð»Ñ Ubuntu):

```yaml
# /etc/netplan/01-netcfg.yaml
network:
  version: 2
  ethernets:
    enp1s0f0:
      dhcp4: no
    enp1s0f1:
      dhcp4: no
    enp2s0f0:
      dhcp4: no
    enp2s0f1:
      dhcp4: no
  
  bonds:
    bond0:  # Public Network
      interfaces: [enp1s0f0, enp1s0f1]
      addresses: [10.0.1.10/24]
      gateway4: 10.0.1.1
      parameters:
        mode: 802.3ad  # LACP
        lacp-rate: fast
        mii-monitor-interval: 100
    
    bond1:  # Cluster Network
      interfaces: [enp2s0f0, enp2s0f1]
      addresses: [10.0.2.10/24]
      parameters:
        mode: 802.3ad
        lacp-rate: fast
        mii-monitor-interval: 100
```

---

## ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹

### Ð’Ñ‹Ð±Ð¾Ñ€ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹

âœ… **Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ðµ Ð´Ð¸ÑÑ‚Ñ€Ð¸Ð±ÑƒÑ‚Ð¸Ð²Ñ‹:**

| ÐžÐ¡ | Ð’ÐµÑ€ÑÐ¸Ñ | Ceph Ð²ÐµÑ€ÑÐ¸Ñ | ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ |
|----|--------|-------------|------------|
| Ubuntu LTS | 22.04 | Reef (18.x) | Ð¡Ð°Ð¼Ñ‹Ð¹ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ |
| RHEL | 9.x | Reef (18.x) | Enterprise Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° |
| Rocky Linux | 9.x | Reef (18.x) | RHEL-ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ð¹, Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ð¾ |

âŒ **Ð˜Ð·Ð±ÐµÐ³Ð°Ñ‚ÑŒ:**
- Ubuntu 14.04/16.04 (ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ðµ, Ð±ÐµÐ· systemd)
- CentOS 7 (EOL)
- Debian stable (ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð¿Ð°ÐºÐµÑ‚Ñ‹)

### Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÐžÐ¡

#### Ð¨Ð°Ð³ 1: Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°

```bash
# ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ (Ubuntu)
apt update && apt upgrade -y

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ñ… Ð¿Ð°ÐºÐµÑ‚Ð¾Ð²
apt install -y \
    python3 \
    python3-pip \
    lvm2 \
    chrony \
    curl \
    wget \
    git \
    vim \
    htop \
    iotop \
    smartmontools \
    nvme-cli

# Ð”Ð»Ñ RHEL/Rocky
dnf update -y
dnf install -y python3 lvm2 chrony curl wget git vim htop iotop smartmontools nvme-cli
```

#### Ð¨Ð°Ð³ 2: Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ (ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž!)

âš ï¸ **ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž**: Ð Ð°ÑÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ > 50ms Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð² Ceph!

```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° chrony
apt install -y chrony

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° /etc/chrony/chrony.conf
cat > /etc/chrony/chrony.conf <<EOF
# Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ NTP ÑÐµÑ€Ð²ÐµÑ€Ñ‹
server ntp1.example.com iburst
server ntp2.example.com iburst
server ntp3.example.com iburst

# Ð Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³
makestep 1.0 3

# Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
logdir /var/log/chrony
EOF

# Ð—Ð°Ð¿ÑƒÑÐº Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ°
systemctl enable --now chrony
systemctl status chrony

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸
chronyc tracking
# Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ:
# Leap status     : Normal
# System time     : 0.000000050 seconds slow of NTP time  â† Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ < 0.05

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²
chronyc sources -v
```

**ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸:**
```bash
# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð² cron Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ
cat > /etc/cron.hourly/check-time-sync <<'EOF'
#!/bin/bash
OFFSET=$(chronyc tracking | grep "System time" | awk '{print $4}')
if (( $(echo "$OFFSET > 0.05" | bc -l) )); then
    echo "WARNING: Time offset $OFFSET seconds" | mail -s "Time Sync Alert" admin@example.com
fi
EOF
chmod +x /etc/cron.hourly/check-time-sync
```

#### Ð¨Ð°Ð³ 3: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÐ´Ñ€Ð° Linux

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð» Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸ ÑÐ´Ñ€Ð°
cat > /etc/sysctl.d/99-ceph-performance.conf <<EOF
# ============================================
# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÐµÑ‚Ð¸ Ð´Ð»Ñ Ceph
# ============================================

# Ð£Ð²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð±ÑƒÑ„ÐµÑ€Ð¾Ð² TCP
net.core.rmem_max = 536870912
net.core.wmem_max = 536870912
net.ipv4.tcp_rmem = 4096 87380 536870912
net.ipv4.tcp_wmem = 4096 65536 536870912

# Ð Ð°Ð·Ð¼ÐµÑ€ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ ÑÐµÑ‚ÐµÐ²Ñ‹Ñ… Ð¿Ð°ÐºÐµÑ‚Ð¾Ð²
net.core.netdev_max_backlog = 300000

# TCP Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸
net.ipv4.tcp_max_syn_backlog = 8192
net.core.somaxconn = 8192
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.tcp_tw_reuse = 1

# ============================================
# Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ
# ============================================

# ÐœÐ¸Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ swapping
vm.swappiness = 10

# Dirty pages (Ð´Ð»Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð¿Ð¸ÑÐ¸)
vm.dirty_ratio = 10
vm.dirty_background_ratio = 5
vm.dirty_expire_centisecs = 1000

# ============================================
# Ð¤Ð°Ð¹Ð»Ð¾Ð²Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°
# ============================================

# Ð›Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ð½Ð° Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
fs.file-max = 1000000
fs.aio-max-nr = 1048576

# ============================================
# ÐŸÑ€Ð¾Ñ†ÐµÑÑÑ‹
# ============================================

# ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ PID (Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð³Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° OSD)
kernel.pid_max = 4194303

# ============================================
# Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
# ============================================

# ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð·Ñ€Ð°Ñ‡Ð½Ñ‹Ñ… huge pages (Ð¼Ð¾Ð¶ÐµÑ‚ Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸)
# Ð”ÐµÐ»Ð°ÐµÑ‚ÑÑ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾, ÑÐ¼. Ð½Ð¸Ð¶Ðµ
EOF

# ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸
sysctl -p /etc/sysctl.d/99-ceph-performance.conf

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ
sysctl net.core.rmem_max
sysctl vm.swappiness
```

**ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Transparent Huge Pages:**
```bash
# THP Ð¼Ð¾Ð¶ÐµÑ‚ Ð²Ñ‹Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸, Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag

# ÐŸÐ¾ÑÑ‚Ð¾ÑÐ½Ð½Ð¾Ðµ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ (Ñ‡ÐµÑ€ÐµÐ· systemd)
cat > /etc/systemd/system/disable-thp.service <<EOF
[Unit]
Description=Disable Transparent Huge Pages (THP)
DefaultDependencies=no
After=sysinit.target local-fs.target
Before=basic.target

[Service]
Type=oneshot
ExecStart=/bin/sh -c 'echo never > /sys/kernel/mm/transparent_hugepage/enabled'
ExecStart=/bin/sh -c 'echo never > /sys/kernel/mm/transparent_hugepage/defrag'

[Install]
WantedBy=basic.target
EOF

systemctl daemon-reload
systemctl enable --now disable-thp.service
```

#### Ð¨Ð°Ð³ 4: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð²

```bash
# Ð£Ð²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð² Ð´Ð»Ñ Ceph Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ¾Ð²
cat > /etc/security/limits.d/99-ceph.conf <<EOF
# Ð›Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ð´Ð»Ñ Ceph Ð´ÐµÐ¼Ð¾Ð½Ð¾Ð²
* soft nofile 1048576
* hard nofile 1048576
* soft nproc unlimited
* hard nproc unlimited
* soft memlock unlimited
* hard memlock unlimited
* soft core unlimited
* hard core unlimited
EOF

# ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑÑ Ð¿Ð¾ÑÐ»Ðµ Ð¿ÐµÑ€ÐµÐ»Ð¾Ð³Ð¸Ð½Ð° Ð¸Ð»Ð¸ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
```

#### Ð¨Ð°Ð³ 5: ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ ÑÐ½ÐµÑ€Ð³Ð¾ÑÐ±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¸Ñ CPU

```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ñ€ÐµÐ¶Ð¸Ð¼Ð°
cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° performance Ñ€ÐµÐ¶Ð¸Ð¼Ð°
apt install -y cpufrequtils

echo 'GOVERNOR="performance"' > /etc/default/cpufrequtils

# ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½ÐµÐ¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

systemctl restart cpufrequtils

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
# Ð’ÑÐµ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ: performance
```

#### Ð¨Ð°Ð³ 6: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð´Ð¸ÑÐºÐ¾Ð²

```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð¸ÑÐºÐ¾Ð²
lsblk
smartctl -a /dev/sda

# ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ñ‰Ð¸ÐºÐ° I/O Ð´Ð»Ñ SSD/NVMe
# Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð´Ð¸ÑÐºÐ°:
echo none > /sys/block/sda/queue/scheduler

# ÐŸÐ¾ÑÑ‚Ð¾ÑÐ½Ð½Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· udev
cat > /etc/udev/rules.d/60-ceph-disk-scheduler.rules <<EOF
# SSD Ð´Ð¸ÑÐºÐ¸ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ none Ð¸Ð»Ð¸ mq-deadline
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="none"

# NVMe Ð´Ð¸ÑÐºÐ¸
ACTION=="add|change", KERNEL=="nvme[0-9]n[0-9]", ATTR{queue/scheduler}="none"

# HDD Ð´Ð¸ÑÐºÐ¸ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ bfq (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/rotational}=="1", ATTR{queue/scheduler}="bfq"
EOF

# ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°
udevadm control --reload-rules
udevadm trigger
```

#### Ð¨Ð°Ð³ 7: Firewall (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ)

```bash
# Ð”Ð»Ñ Ubuntu (ufw)
ufw allow from 10.0.1.0/24  # Public network
ufw allow from 10.0.2.0/24  # Cluster network

# Ð”Ð»Ñ RHEL/Rocky (firewalld)
firewall-cmd --permanent --add-service=ceph
firewall-cmd --permanent --add-service=ceph-mon
firewall-cmd --reload

# Ð˜Ð»Ð¸ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ firewall Ð½Ð° cluster network (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ)
# Cluster network Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð¸Ð·Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð°
```

---

## Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ceph

### Ð’Ñ‹Ð±Ð¾Ñ€ Ð¼ÐµÑ‚Ð¾Ð´Ð° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸

| ÐœÐµÑ‚Ð¾Ð´ | Ð’ÐµÑ€ÑÐ¸Ñ Ceph | Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ | ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ |
|-------|-------------|--------------|------------|
| **cephadm** | Octopus+ (15+) | âœ… Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ | ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹, ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñ‹, Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ |
| ceph-ansible | Ð’ÑÐµ Ð²ÐµÑ€ÑÐ¸Ð¸ | âš ï¸ Legacy | Ð”Ð»Ñ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð²ÐµÑ€ÑÐ¸Ð¹ |
| Rook | Ð’ÑÐµ Ð²ÐµÑ€ÑÐ¸Ð¸ | ðŸ”§ Kubernetes | Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ K8s Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ð¹ |
| Ð ÑƒÑ‡Ð½Ð°Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° | Ð’ÑÐµ Ð²ÐµÑ€ÑÐ¸Ð¸ | âŒ ÐÐµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ | Ð¡Ð»Ð¾Ð¶Ð½Ð¾ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ñ‚ÑŒ |

### Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· cephadm (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ)

#### Ð¨Ð°Ð³ 1: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° cephadm Ð½Ð° Ð¿ÐµÑ€Ð²Ð¾Ð¼ ÑƒÐ·Ð»Ðµ

```bash
# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½Ð° Ubuntu
apt install -y cephadm

# Ð˜Ð›Ð˜ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ Ð²ÐµÑ€ÑÐ¸Ð¸
curl --silent --remote-name --location \
  https://download.ceph.com/rpm-reef/el9/noarch/cephadm

chmod +x cephadm
mv cephadm /usr/local/bin/

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²ÐµÑ€ÑÐ¸Ð¸
cephadm --version
```

#### Ð¨Ð°Ð³ 2: Bootstrap Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑƒÐ·Ð»Ð°

âš ï¸ **ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¿ÐµÑ€ÐµÐ´ bootstrap:**

```bash
# Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ Ñ‡Ñ‚Ð¾:
# 1. Hostname Ñ€Ð°Ð·Ñ€ÐµÑˆÐ°ÐµÑ‚ÑÑ Ð² IP
hostnamectl set-hostname node1.ceph.local
echo "10.0.1.11 node1.ceph.local node1" >> /etc/hosts

# 2. ÐŸÐ¾Ñ€Ñ‚Ñ‹ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ñ‹ (3300, 6789, 6800-7300, 8443, 9283)
ss -tlnp | grep -E '3300|6789|8443|9283'

# 3. Docker/Podman ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½ (cephadm ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸)

# 4. Ð”Ð¸ÑÐºÐ¸ Ñ‡Ð¸ÑÑ‚Ñ‹Ðµ (Ð±ÐµÐ· Ð¿Ð°Ñ€Ñ‚Ð¸Ñ†Ð¸Ð¹)
lsblk
# ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð¸ÑÐºÐ¾Ð² Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸:
# wipefs -a /dev/sdb
# dd if=/dev/zero of=/dev/sdb bs=1M count=100
```

**Bootstrap ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°:**

```bash
# Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ bootstrap
cephadm bootstrap \
  --mon-ip 10.0.1.11 \
  --cluster-network 10.0.2.0/24 \
  --initial-dashboard-user admin \
  --initial-dashboard-password 'StrongP@ssw0rd!' \
  --allow-fqdn-hostname \
  --single-host-defaults

# Ð˜Ð›Ð˜ Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸ Ð´Ð»Ñ production
cephadm bootstrap \
  --mon-ip 10.0.1.11 \
  --cluster-network 10.0.2.0/24 \
  --initial-dashboard-user admin \
  --initial-dashboard-password 'StrongP@ssw0rd!' \
  --allow-fqdn-hostname \
  --dashboard-password-noupdate \
  --ssh-user root \
  --skip-monitoring-stack  # ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ð¼ Ð¿Ð¾Ð·Ð¶Ðµ Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸

# ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð·Ð°Ð¹Ð¼ÐµÑ‚ 5-10 Ð¼Ð¸Ð½ÑƒÑ‚
# Ð’ ÐºÐ¾Ð½Ñ†Ðµ Ð±ÑƒÐ´ÐµÑ‚ Ð²Ñ‹Ð²ÐµÐ´ÐµÐ½Ð° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ:
# - URL dashboard: https://node1:8443
# - Ð›Ð¾Ð³Ð¸Ð½: admin
# - ÐŸÐ°Ñ€Ð¾Ð»ÑŒ: StrongP@ssw0rd!
# - SSH Ð¿ÑƒÐ±Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑƒÐ·Ð»Ð¾Ð²
```

**Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð²Ð°Ð¶Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ:**
```bash
# Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
mkdir -p /root/ceph-install
ceph config dump > /root/ceph-install/initial-config.txt

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ SSH ÐºÐ»ÑŽÑ‡
cp /etc/ceph/ceph.pub /root/ceph-install/

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ admin ÐºÐ»ÑŽÑ‡
ceph auth get client.admin > /root/ceph-install/admin-keyring.txt
```

#### Ð¨Ð°Ð³ 3: ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑƒÐ·Ð»Ð¾Ð²

```bash
# ÐÐ° Ð²ÑÐµÑ… Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑƒÐ·Ð»Ð°Ñ… (node2, node3, ...):

# 1. ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¸Ð· Ñ€Ð°Ð·Ð´ÐµÐ»Ð° "ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹"

# 2. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ hostname Ð¸ hosts
hostnamectl set-hostname node2.ceph.local
cat >> /etc/hosts <<EOF
10.0.1.11 node1.ceph.local node1
10.0.1.12 node2.ceph.local node2
10.0.1.13 node3.ceph.local node3
EOF

# 3. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Docker/Podman (ÐµÑÐ»Ð¸ Ð½ÐµÑ‚)
apt install -y docker.io
systemctl enable --now docker
```

#### Ð¨Ð°Ð³ 4: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ·Ð»Ð¾Ð² Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€

```bash
# ÐÐ° Ð¿ÐµÑ€Ð²Ð¾Ð¼ ÑƒÐ·Ð»Ðµ (node1):

# ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ SSH ÐºÐ»ÑŽÑ‡ÐµÐ¹
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node2
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° SSH Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð°
ssh root@node2 hostname
ssh root@node3 hostname

# Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ·Ð»Ð¾Ð² Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€
ceph orch host add node2 10.0.1.12 --labels _admin
ceph orch host add node3 10.0.1.13 --labels _admin

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¿Ð¸ÑÐºÐ° ÑƒÐ·Ð»Ð¾Ð²
ceph orch host ls

# Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
# HOST   ADDR         LABELS  STATUS
# node1  10.0.1.11    _admin  
# node2  10.0.1.12    _admin  
# node3  10.0.1.13    _admin  
```

#### Ð¨Ð°Ð³ 5: Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ MON Ð¸ MGR Ð´ÐµÐ¼Ð¾Ð½Ð¾Ð²

```bash
# ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ðµ MON (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð½ÐµÑ‡ÐµÑ‚Ð½Ð¾Ðµ Ñ‡Ð¸ÑÐ»Ð¾: 3, 5, 7)
ceph orch apply mon --placement="3 node1 node2 node3"

# ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ðµ MGR (Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 2 Ð´Ð»Ñ HA)
ceph orch apply mgr --placement="3 node1 node2 node3"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ñ
ceph orch ps --daemon-type mon
ceph orch ps --daemon-type mgr

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ²Ð¾Ñ€ÑƒÐ¼Ð°
ceph mon stat
# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ: e2: 3 mons at {...}, election epoch X, leader X, quorum 0,1,2

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° MGR
ceph mgr stat
# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ: Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ MGR Ð¸ standby
```

#### Ð¨Ð°Ð³ 6: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° CRUSH Map (Ñ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ñ)

âš ï¸ **ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž**: CRUSH Map Ð´Ð¾Ð»Ð¶Ð½Ð° ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ñ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ð¸!

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹ ÑÑ‚Ð¾ÐµÐº (racks)
ceph osd crush add-bucket rack1 rack
ceph osd crush add-bucket rack2 rack
ceph osd crush add-bucket rack3 rack

# ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰ÐµÐ½Ð¸Ðµ Ñ…Ð¾ÑÑ‚Ð¾Ð² Ð² ÑÑ‚Ð¾Ð¹ÐºÐ¸
ceph osd crush move node1 rack=rack1
ceph osd crush move node2 rack=rack2
ceph osd crush move node3 rack=rack3

# ÐŸÐµÑ€ÐµÐ¼ÐµÑ‰ÐµÐ½Ð¸Ðµ ÑÑ‚Ð¾ÐµÐº Ð¿Ð¾Ð´ root
ceph osd crush move rack1 root=default
ceph osd crush move rack2 root=default
ceph osd crush move rack3 root=default

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ð¸
ceph osd crush tree

# Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ:
# ID   CLASS  WEIGHT   TYPE NAME         
# -1          0.00000  root default      
# -3          0.00000      rack rack1    
# -2          0.00000          host node1
# -5          0.00000      rack rack2    
# -4          0.00000          host node2
# -7          0.00000      rack rack3    
# -6          0.00000          host node3
```

**Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð¾Ñ‚ÐºÐ°Ð·Ð¾ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚Ð¸:**

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾: Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð° Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÑ‚Ð¾Ð¹ÐºÐ°Ñ…
ceph osd crush rule create-replicated replicated_rack \
  default rack host

# ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð¿Ñ€Ð°Ð²Ð¸Ð»
ceph osd crush rule ls
ceph osd crush rule dump replicated_rack
```

#### Ð¨Ð°Ð³ 7: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ OSD (Ð´Ð¸ÑÐºÐ¾Ð²)

**ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²:**

```bash
# ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð²ÑÐµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð´Ð¸ÑÐºÐ¸ Ð²Ð¾ Ð²ÑÐµÐ¼ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ðµ
ceph orch device ls

# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð²Ñ‹Ð²Ð¾Ð´Ð°:
# Hostname  Path      Type  Serial           Size   Health  Ident  Fault  Available
# node1     /dev/sdb  ssd   S455NX0M123456   3.8T   Unknown  N/A    N/A    Yes
# node1     /dev/sdc  ssd   S455NX0M123457   3.8T   Unknown  N/A    N/A    Yes
# node2     /dev/sdb  ssd   S455NX0M123458   3.8T   Unknown  N/A    N/A    Yes
# ...

# ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð´Ð¸ÑÐºÐ¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ ÑƒÐ·Ð»Ð°
ceph orch device ls --hostname=node1
```

**Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ OSD - Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 1: ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð²ÑÐµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ:**

```bash
# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð´Ð¸ÑÐºÐ¸ Ð²Ð¾ Ð²ÑÐµÐ¼ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ðµ
ceph orch apply osd --all-available-devices

# Ð˜Ð›Ð˜ Ð½Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ñ…Ð¾ÑÑ‚Ð°Ñ…
ceph orch apply osd --all-available-devices --hostname=node1
ceph orch apply osd --all-available-devices --hostname=node2
ceph orch apply osd --all-available-devices --hostname=node3

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°
ceph orch ls osd
ceph -w  # watch mode, Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ
```

**Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ OSD - Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2: Ð’Ñ€ÑƒÑ‡Ð½ÑƒÑŽ (ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð¾):**

```bash
# Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… Ð´Ð¸ÑÐºÐ¾Ð²
ceph orch daemon add osd node1:/dev/sdb
ceph orch daemon add osd node1:/dev/sdc
ceph orch daemon add osd node1:/dev/sdd
# ... Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð´Ð¸ÑÐºÐ¾Ð²

# ÐœÐ°ÑÑÐ¾Ð²Ð¾Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· ÑÐºÑ€Ð¸Ð¿Ñ‚
for node in node1 node2 node3; do
  for device in /dev/sd{b..k}; do
    ceph orch daemon add osd $node:$device
  done
done
```

**ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… OSD:**

```bash
# ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð²ÑÐµÑ… OSD
ceph osd tree

# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð²Ñ‹Ð²Ð¾Ð´Ð°:
# ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
# -1         90.00000  root default                              
# -3         30.00000      rack rack1                            
# -2         30.00000          host node1                        
#  0   ssd    3.00000              osd.0     up   1.00000  1.00000
#  1   ssd    3.00000              osd.1     up   1.00000  1.00000
# ...

# Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° OSD
ceph osd stat
# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ: X osds: X up, X in

# Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ
ceph osd df tree
```

#### Ð¨Ð°Ð³ 8: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿ÑƒÐ»Ð¾Ð² Ð´Ð°Ð½Ð½Ñ‹Ñ…

**âš ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸!**

```bash
# ÐÐ˜ÐšÐžÐ“Ð”Ð ÐÐ• Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð™Ð¢Ð• size=1 !
# Ð­Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÑ‚ Ðº Ð¿Ð¾Ñ‚ÐµÑ€Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸ Ð¾Ñ‚ÐºÐ°Ð·Ðµ Ð´Ð¸ÑÐºÐ°

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿ÑƒÐ»Ð° Ð´Ð»Ñ general purpose
ceph osd pool create mypool 128 128 replicated

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ€ÐµÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
ceph osd pool set mypool size 3        # 3 ÐºÐ¾Ð¿Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž!)
ceph osd pool set mypool min_size 2    # Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 2 ÐºÐ¾Ð¿Ð¸Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ñ
ceph osd pool set mypool crush_rule replicated_rack

# Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ PG
ceph osd pool set mypool pg_autoscale_mode on

# Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð´Ð»Ñ RBD)
ceph osd pool application enable mypool rbd

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº Ð¿ÑƒÐ»Ð°
ceph osd pool get mypool all
```

**Ð Ð°ÑÑ‡ÐµÑ‚ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° PG (Placement Groups):**

Ð”Ð»Ñ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ð²ÐµÑ€ÑÐ¸Ð¹ Ceph Ð±ÐµÐ· pg_autoscale:

```bash
# Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð°: (OSD Ã— 100) / replica_size / pool_count
# ÐŸÑ€Ð¸Ð¼ÐµÑ€: (30 OSD Ã— 100) / 3 / 2 = 500 PG

# ÐžÐºÑ€ÑƒÐ³Ð»Ð¸Ñ‚ÑŒ Ð´Ð¾ ÑÑ‚ÐµÐ¿ÐµÐ½Ð¸ 2: 512

ceph osd pool create mypool 512 512 replicated
```

**Ð¢Ð¸Ð¿Ð¾Ð²Ñ‹Ðµ Ð¿ÑƒÐ»Ñ‹:**

```bash
# RBD Ð¿ÑƒÐ» (Ð´Ð»Ñ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð°ÑˆÐ¸Ð½, Kubernetes)
ceph osd pool create rbd-pool 256 256 replicated
ceph osd pool set rbd-pool size 3 min_size 2
ceph osd pool application enable rbd-pool rbd

# CephFS Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
ceph osd pool create cephfs-metadata 128 128 replicated
ceph osd pool set cephfs-metadata size 3 min_size 2

# CephFS Ð´Ð°Ð½Ð½Ñ‹Ðµ
ceph osd pool create cephfs-data 512 512 replicated
ceph osd pool set cephfs-data size 3 min_size 2

# S3/RadosGW Ð¿ÑƒÐ»Ñ‹
ceph osd pool create rgw-root 32 32 replicated
ceph osd pool create rgw-data 512 512 replicated
ceph osd pool set rgw-data size 3 min_size 2
```

#### Ð¨Ð°Ð³ 9: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‚Ð¾ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸

```bash
# ÐžÐ±Ñ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
ceph -s

# ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
#   cluster:
#     id:     abc123-def456-...
#     health: HEALTH_OK     â† Ð’ÐÐ–ÐÐž!
# 
#   services:
#     mon: 3 daemons, quorum node1,node2,node3
#     mgr: node1(active, since 1h), standbys: node2, node3
#     osd: 30 osds: 30 up, 30 in
# 
#   data:
#     pools:   1 pools, 128 pgs
#     objects: 0 objects, 0 B
#     usage:   900 MiB used, 120 TiB / 120 TiB avail
#     pgs:     128 active+clean

# Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ
ceph health detail

# Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ warnings
ceph health detail
# ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ warning!

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²ÐµÑ€ÑÐ¸Ð¹
ceph versions

# Ð¢ÐµÑÑ‚ Ð·Ð°Ð¿Ð¸ÑÐ¸/Ñ‡Ñ‚ÐµÐ½Ð¸Ñ
rados -p mypool bench 10 write --no-cleanup
rados -p mypool bench 10 seq
rados -p mypool bench 10 rand
```

---

## ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¸ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ

### Ð‘Ð°Ð·Ð¾Ð²Ð°Ñ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ceph

#### ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð´Ð²ÑƒÑ… ÑÐµÑ‚ÐµÐ¹

```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚ÐµÐºÑƒÑ‰Ð¸Ñ… Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
ceph config get mon public_network
ceph config get mon cluster_network

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐµÑ‚ÐµÐ¹ (ÐµÑÐ»Ð¸ Ð½Ðµ Ð±Ñ‹Ð»Ð¾ ÑÐ´ÐµÐ»Ð°Ð½Ð¾ Ð¿Ñ€Ð¸ bootstrap)
ceph config set mon public_network 10.0.1.0/24
ceph config set mon cluster_network 10.0.2.0/24

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ
ceph config dump | grep network
```

#### ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ñ‰Ð¸Ñ… Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²

```bash
# ============================================
# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ (recovery)
# ============================================

# ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: Ð·Ð°Ð¼ÐµÐ´Ð»Ð¸Ñ‚ÑŒ recovery Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÑƒÐ±Ð¸Ñ‚ÑŒ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð²
ceph config set osd osd_max_backfills 1              # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ 1 backfill Ð½Ð° OSD
ceph config set osd osd_recovery_max_active 1        # Ñ‚Ð¾Ð»ÑŒÐºÐ¾ 1 recovery Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ñ
ceph config set osd osd_recovery_op_priority 1       # Ð½Ð¸Ð·ÐºÐ¸Ð¹ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚
ceph config set osd osd_recovery_sleep 0.1           # Ð¿Ð°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸ (ÑÐµÐº)

# Ð”Ð»Ñ HDD ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ Ð¿Ð°ÑƒÐ·Ñƒ
ceph config set osd osd_recovery_sleep_hdd 0.2

# Ð”Ð»Ñ SSD Ð¼ÐµÐ½ÑŒÑˆÐ°Ñ Ð¿Ð°ÑƒÐ·Ð°
ceph config set osd osd_recovery_sleep_ssd 0.05

# ============================================
# Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ñ‹ Ð¾Ñ‚ÐºÐ°Ð·Ð° OSD
# ============================================

# ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð»Ð¾Ð¶Ð½Ñ‹Ñ… ÑÑ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ð¹
ceph config set global mon_osd_down_out_interval 3600   # 1 Ñ‡Ð°Ñ Ð²Ð¼ÐµÑÑ‚Ð¾ 600 ÑÐµÐº (10 Ð¼Ð¸Ð½)

# Ð’Ñ€ÐµÐ¼Ñ Ð´Ð¾Ð¾Ð¼ÐµÑ‚ÐºÐ¸ OSD ÐºÐ°Ðº down
ceph config set global mon_osd_report_timeout 900       # 15 Ð¼Ð¸Ð½ÑƒÑ‚

# ============================================
# ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ debug Ð»Ð¾Ð³Ð¾Ð² (Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ)
# ============================================

ceph config set global debug_ms 0
ceph config set global debug_mon 0
ceph config set global debug_osd 0
ceph config set global debug_bluestore 0
ceph config set global debug_rocksdb 0

# ============================================
# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÐºÑ€Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸)
# ============================================

# Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÑ‚ÑŒ ÑÐºÑ€Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð½Ð¾Ñ‡ÑŒÑŽ
ceph config set osd osd_scrub_begin_hour 1      # Ð½Ð°Ñ‡Ð°Ð»Ð¾ Ð² 01:00
ceph config set osd osd_scrub_end_hour 6        # ÐºÐ¾Ð½ÐµÑ† Ð² 06:00

# ÐÐ°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ñ€Ð¸ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ ÐÐ• ÑÐºÑ€Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ
ceph config set osd osd_scrub_load_threshold 0.5

# Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ñ‹ ÑÐºÑ€Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
ceph config set osd osd_scrub_min_interval 86400          # Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 1 Ñ€Ð°Ð· Ð² Ð´ÐµÐ½ÑŒ
ceph config set osd osd_scrub_max_interval 604800         # Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ Ñ€Ð°Ð· Ð² Ð½ÐµÐ´ÐµÐ»ÑŽ
ceph config set osd osd_deep_scrub_interval 1209600       # deep scrub Ñ€Ð°Ð· Ð² 2 Ð½ÐµÐ´ÐµÐ»Ð¸

# ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ ÑÐºÑ€Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
ceph config set osd osd_scrub_priority 1                  # Ð½Ð¸Ð·ÐºÐ¸Ð¹ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚
ceph config set osd osd_scrub_sleep 0.1                   # Ð¿Ð°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÐ¼Ð¸
```

#### ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ SSD

```bash
# ============================================
# BlueStore Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ SSD
# ============================================

# Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÑÑˆÐ° Ð´Ð»Ñ SSD (3GB Ð½Ð° OSD)
ceph config set osd bluestore_cache_size_ssd 3221225472

# ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹
ceph config set osd bluestore_max_ops 4096
ceph config set osd bluestore_max_bytes 268435456

# ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ RocksDB Ð´Ð»Ñ SSD
ceph config set osd bluestore_rocksdb_options \
  "compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4,write_buffer_size=268435456,writable_file_max_buffer_size=0,compaction_readahead_size=2097152,max_background_compactions=2"

# ============================================
# OSD Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
# ============================================

# Ð£Ð²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼Ð°
ceph config set osd osd_op_num_threads_per_shard 2
ceph config set osd osd_op_num_shards 8

# Client Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
ceph config set osd osd_client_message_size_cap 524288000
```

#### ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ HDD (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ)

```bash
# Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÑÑˆÐ° Ð´Ð»Ñ HDD (1GB Ð½Ð° OSD)
ceph config set osd bluestore_cache_size_hdd 1073741824

# ÐœÐµÐ½ÑŒÑˆÐµ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼Ð°
ceph config set osd osd_op_num_shards 5
```

#### ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²

```bash
# ============================================
# ÐÐ³Ñ€ÐµÑÑÐ¸Ð²Ð½Ñ‹Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²
# Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð¬ ÐžÐ¡Ð¢ÐžÐ ÐžÐ–ÐÐž!
# ============================================

# Ð‘Ð¾Ð»ÑŒÑˆÐ¸Ðµ Ð±ÑƒÑ„ÐµÑ€Ñ‹
ceph config set osd osd_client_message_size_cap 1073741824

# Ð‘Ð¾Ð»ÑŒÑˆÐµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾
ceph config set osd osd_max_backfills 2
ceph config set osd osd_recovery_max_active 3

# ÐœÐµÐ½ÑŒÑˆÐµ Ð·Ð°Ð´ÐµÑ€Ð¶ÐµÐº
ceph config set osd osd_recovery_sleep 0.01
ceph config set osd osd_recovery_sleep_ssd 0

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° heartbeat
ceph config set osd osd_heartbeat_grace 20
ceph config set osd osd_heartbeat_interval 6
```

### ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² Ñ‡ÐµÑ€ÐµÐ· systemd

**âš ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ OOM!**

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ override Ð´Ð»Ñ OSD ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
# Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ OSD Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾

# Ð£Ð·Ð½Ð°Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº OSD Ð½Ð° ÑƒÐ·Ð»Ðµ
systemctl list-units 'ceph-osd@*.service'

# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ override Ð´Ð»Ñ OSD
systemctl edit ceph-osd@0.service

# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ:
[Service]
# ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¼ÑÑ‚Ð¸
MemoryMax=8G          # Ð¶ÐµÑÑ‚ÐºÐ¸Ð¹ Ð»Ð¸Ð¼Ð¸Ñ‚ (OSD Ð±ÑƒÐ´ÐµÑ‚ ÑƒÐ±Ð¸Ñ‚ Ð¿Ñ€Ð¸ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¸Ð¸)
MemoryHigh=7G         # Ð¼ÑÐ³ÐºÐ¸Ð¹ Ð»Ð¸Ð¼Ð¸Ñ‚ (Ð½Ð°Ñ‡Ð½ÐµÑ‚ÑÑ throttling)

# ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ CPU
CPUQuota=200%         # Ð½Ðµ Ð±Ð¾Ð»ÐµÐµ 2 ÑÐ´ÐµÑ€

# ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ I/O (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
IOWeight=100          # Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ I/O

# ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð´Ð»Ñ Ð²ÑÐµÑ… OSD
for osd_id in {0..9}; do
  systemctl edit ceph-osd@${osd_id}.service
  # Ð²ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ Ð²Ñ‹ÑˆÐµ
done

# ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ systemd
systemctl daemon-reload

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð²
systemctl show ceph-osd@0.service | grep -E 'Memory|CPU'
```

**Ð£Ð²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð² Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ recovery:**

```bash
# Ð’Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¼Ð°ÑÑÐ¾Ð²Ð¾Ð³Ð¾ recovery Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ
systemctl edit ceph-osd@0.service

[Service]
MemoryMax=16G
MemoryHigh=14G

systemctl daemon-reload
systemctl restart ceph-osd@0.service

# ÐŸÐžÐ¡Ð›Ð• Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ recovery Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾!
```

### ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ„Ð»Ð°Ð³Ð¾Ð² Ð´Ð»Ñ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ

```bash
# ============================================
# Ð¤Ð»Ð°Ð³Ð¸ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ð³Ð¾ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ
# Ð’Ð¡Ð•Ð“Ð”Ð ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ñ‚ÑŒ ÐŸÐ•Ð Ð•Ð” Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸ÐµÐ¼!
# ============================================

# Ð—Ð°Ð¿Ñ€ÐµÑ‚Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð¼ÐµÑ‡Ð°Ñ‚ÑŒ OSD ÐºÐ°Ðº out Ð¿Ñ€Ð¸ Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¸
ceph osd set noout

# Ð—Ð°Ð¿Ñ€ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð°Ñ‡Ð¸Ð½Ð°Ñ‚ÑŒ recovery
ceph osd set norecover

# Ð—Ð°Ð¿Ñ€ÐµÑ‚Ð¸Ñ‚ÑŒ backfill
ceph osd set nobackfill

# Ð—Ð°Ð¿Ñ€ÐµÑ‚Ð¸Ñ‚ÑŒ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÑƒ
ceph osd set norebalance

# Ð—Ð°Ð¿Ñ€ÐµÑ‚Ð¸Ñ‚ÑŒ deep scrub
ceph osd set nodeep-scrub

# Ð—Ð°Ð¿Ñ€ÐµÑ‚Ð¸Ñ‚ÑŒ scrub
ceph osd set noscrub

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ñ„Ð»Ð°Ð³Ð¾Ð²
ceph osd dump | grep flags

# ÐŸÐžÐ¡Ð›Ð• Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ ÐžÐ‘Ð¯Ð—ÐÐ¢Ð•Ð›Ð¬ÐÐž ÑÐ½ÑÑ‚ÑŒ:
ceph osd unset noout
ceph osd unset norecover
ceph osd unset nobackfill
ceph osd unset norebalance
ceph osd unset nodeep-scrub
ceph osd unset noscrub
```

**Ð¢Ð¸Ð¿Ð¾Ð²Ñ‹Ðµ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ„Ð»Ð°Ð³Ð¾Ð²:**

```bash
# Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ 1: Ð—Ð°Ð¼ÐµÐ½Ð° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð´Ð¸ÑÐºÐ°
ceph osd set noout norebalance nobackfill
# ... Ð·Ð°Ð¼ÐµÐ½Ð° Ð´Ð¸ÑÐºÐ° ...
ceph osd unset noout norebalance nobackfill

# Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ 2: ÐŸÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑƒÐ·Ð»Ð°
ceph osd set noout norecover nobackfill
# ... Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑƒÐ·Ð»Ð° ...
ceph osd unset noout norecover nobackfill

# Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ 3: ÐŸÐ»Ð°Ð½Ð¾Ð²Ð¾Ðµ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ (Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ñ†ÐµÐ»Ð¾Ð¹ ÑÑ‚Ð¾Ð¹ÐºÐ¸)
ceph osd set noout norecover nobackfill norebalance
# ... Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ ...
ceph osd unset noout norecover nobackfill norebalance

# Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ 4: ÐÐ¾Ñ‡Ð½Ð¾Ðµ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ (Ñ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼)
ceph osd set noout norebalance
# ... Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ ...
ceph osd unset noout norebalance
# recovery Ð½Ð°Ñ‡Ð½ÐµÑ‚ÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
```

---

## ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°

### Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑƒÐ·Ð»Ð¾Ð²

#### Ð˜ÑÑ…Ð¾Ð´Ð½Ð°Ñ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ: 3 ÑƒÐ·Ð»Ð°

```
ÐÐ°Ñ‡Ð°Ð»Ð¾: 3 ÑƒÐ·Ð»Ð° Ã— 10 OSD = 30 OSD
Ð•Ð¼ÐºÐ¾ÑÑ‚ÑŒ: 30 Ã— 4TB = 120TB raw = 40TB usable (replica 3)
```

### Ð­Ñ‚Ð°Ð¿ 1: Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ 3 â†’ 5 ÑƒÐ·Ð»Ð¾Ð²

#### Ð Ð°ÑÑ‡ÐµÑ‚ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸

```
Ð‘Ñ‹Ð»Ð¾: 30 OSD
Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼: 20 OSD (2 ÑƒÐ·Ð»Ð° Ã— 10 OSD)
Ð¡Ñ‚Ð°Ð½ÐµÑ‚: 50 OSD

ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸ = (20 / 50) Ã— 100% = 40% Ð´Ð°Ð½Ð½Ñ‹Ñ…

ÐŸÑ€Ð¸ 40TB Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿ÐµÑ€ÐµÐ¼ÐµÑÑ‚Ð¸Ñ‚ÑÑ ~16TB
Ð’Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¸ 10 Gbit/s: ~4-5 Ð´Ð½ÐµÐ¹
Ð’Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¸ 25 Gbit/s: ~1.5-2 Ð´Ð½Ñ
```

#### Ð¨Ð°Ð³ 1: ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð½Ð¾Ð²Ñ‹Ñ… ÑƒÐ·Ð»Ð¾Ð²

```bash
# ÐÐ° Ð½Ð¾Ð²Ñ‹Ñ… ÑƒÐ·Ð»Ð°Ñ… (node4, node5):

# 1. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÐžÐ¡ Ubuntu 22.04
# 2. ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð’Ð¡Ð• Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¸Ð· Ñ€Ð°Ð·Ð´ÐµÐ»Ð° "ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸Ð½Ñ„Ñ€Ð°ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹":
#    - Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ (chrony)
#    - ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÐ´Ñ€Ð° (sysctl)
#    - Ð›Ð¸Ð¼Ð¸Ñ‚Ñ‹ (limits.conf)
#    - ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ ÑÐ½ÐµÑ€Ð³Ð¾ÑÐ±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¸Ñ CPU
#    - ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÐµÑ‚Ð¸ (2 ÑÐµÑ‚Ð¸!)

# 3. ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ hostname Ð¸ /etc/hosts
hostnamectl set-hostname node4.ceph.local

cat >> /etc/hosts <<EOF
10.0.1.11 node1.ceph.local node1
10.0.1.12 node2.ceph.local node2
10.0.1.13 node3.ceph.local node3
10.0.1.14 node4.ceph.local node4
10.0.1.15 node5.ceph.local node5
EOF

# 4. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
chronyc tracking
# System time Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ < 0.05 ÑÐµÐºÑƒÐ½Ð´!

# 5. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐµÑ‚Ð¸
ping -c 3 10.0.1.11  # public network
ping -c 3 10.0.2.11  # cluster network
```

#### Ð¨Ð°Ð³ 2: ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž - Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸ ÐŸÐ•Ð Ð•Ð” Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼!

```bash
# ÐÐ° Ð¼Ð°ÑÑ‚ÐµÑ€-ÑƒÐ·Ð»Ðµ (node1):

# âš ï¸ Ð‘Ð•Ð— Ð­Ð¢ÐžÐ“Ðž Ceph Ð½Ð°Ñ‡Ð½ÐµÑ‚ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÑƒ Ð¡Ð ÐÐ—Ð£!
ceph osd set noout
ceph osd set norebalance
ceph osd set nobackfill

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph osd dump | grep flags
# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ: noout,norebalance,nobackfill

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ
ceph -s
```

#### Ð¨Ð°Ð³ 3: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ·Ð»Ð¾Ð² Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€

```bash
# ÐÐ° Ð¼Ð°ÑÑ‚ÐµÑ€-ÑƒÐ·Ð»Ðµ (node1):

# ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ SSH ÐºÐ»ÑŽÑ‡ÐµÐ¹
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node4
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node5

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° SSH
ssh root@node4 'hostname && date'
ssh root@node5 'hostname && date'
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ Ð²Ñ€ÐµÐ¼Ñ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾!

# Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ·Ð»Ð¾Ð²
ceph orch host add node4 10.0.1.14 --labels _admin
ceph orch host add node5 10.0.1.15 --labels _admin

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph orch host ls

# Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
# HOST   ADDR         LABELS  STATUS
# node1  10.0.1.11    _admin  
# node2  10.0.1.12    _admin  
# node3  10.0.1.13    _admin  
# node4  10.0.1.14    _admin   â† Ð½Ð¾Ð²Ñ‹Ð¹
# node5  10.0.1.15    _admin   â† Ð½Ð¾Ð²Ñ‹Ð¹
```

#### Ð¨Ð°Ð³ 4: ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ CRUSH Map

```bash
# Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ðµ ÑÑ‚Ð¾Ð¹ÐºÐ¸
ceph osd crush add-bucket rack4 rack
ceph osd crush add-bucket rack5 rack

# ÐŸÐµÑ€ÐµÐ¼ÐµÑÑ‚Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ðµ Ñ…Ð¾ÑÑ‚Ñ‹ Ð² ÑÑ‚Ð¾Ð¹ÐºÐ¸
ceph osd crush move node4 rack=rack4
ceph osd crush move node5 rack=rack5

# ÐŸÐµÑ€ÐµÐ¼ÐµÑÑ‚Ð¸Ñ‚ÑŒ ÑÑ‚Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾Ð´ root
ceph osd crush move rack4 root=default
ceph osd crush move rack5 root=default

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð³Ð¸Ð¸
ceph osd crush tree

# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ:
# ID   CLASS  WEIGHT   TYPE NAME         
# -1          120.00   root default      
# -3           30.00       rack rack1    
# -2           30.00           host node1
# -5           30.00       rack rack2    
# -4           30.00           host node2
# -7           30.00       rack rack3    
# -6           30.00           host node3
# -9            0.00       rack rack4    â† Ð½Ð¾Ð²Ñ‹Ð¹
# -8            0.00           host node4
# -11           0.00       rack rack5    â† Ð½Ð¾Ð²Ñ‹Ð¹
# -10           0.00           host node5
```

#### Ð¨Ð°Ð³ 5: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ MON Ð´ÐµÐ¼Ð¾Ð½Ð¾Ð² (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)

```bash
# ÐŸÑ€Ð¸ 5 ÑƒÐ·Ð»Ð°Ñ… Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ 5 MON

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚ÐµÐºÑƒÑ‰Ð¸Ñ… MON
ceph mon stat
# e2: 3 mons at {...}

# Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ MON Ð½Ð° Ð½Ð¾Ð²Ñ‹Ðµ ÑƒÐ·Ð»Ñ‹
ceph orch apply mon --placement="5 node1 node2 node3 node4 node5"

# Ð˜Ð›Ð˜ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
ceph orch apply mon 5

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph mon stat
# e4: 5 mons at {...}

ceph orch ps --daemon-type mon
```

#### Ð¨Ð°Ð³ 6: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ OSD

```bash
# ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð¸ÑÐºÐ¾Ð² Ð½Ð° Ð½Ð¾Ð²Ñ‹Ñ… ÑƒÐ·Ð»Ð°Ñ…
ceph orch device ls --hostname=node4
ceph orch device ls --hostname=node5

# Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð¸ÑÐºÐ¾Ð²
ceph orch apply osd --all-available-devices --hostname=node4
ceph orch apply osd --all-available-devices --hostname=node5

# Ð˜Ð›Ð˜ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð´Ð¸ÑÐºÐ°
for device in /dev/sd{b..k}; do
  ceph orch daemon add osd node4:$device
  ceph orch daemon add osd node5:$device
done

# ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ (Ð·Ð°Ð¹Ð¼ÐµÑ‚ 5-15 Ð¼Ð¸Ð½ÑƒÑ‚)
watch -n 10 'ceph osd tree'

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‡Ñ‚Ð¾ Ð²ÑÐµ OSD Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹
ceph osd tree
# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ 50 OSD: 50 up, 50 in

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° CRUSH Ð²ÐµÑÐ¾Ð²
ceph osd df tree
# Ð’ÑÐµ ÑƒÐ·Ð»Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¸Ð¼ÐµÑ‚ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ð¹ WEIGHT
```

#### Ð¨Ð°Ð³ 7: ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð°Ñ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ°

**âš ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐ«Ð™ Ð­Ð¢ÐÐŸ: ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° recovery!**

```bash
# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸
# Ð”Ð›Ð¯ ÐœÐ˜ÐÐ˜ÐœÐÐ›Ð¬ÐÐžÐ“Ðž Ð’Ð›Ð˜Ð¯ÐÐ˜Ð¯ ÐÐ ÐšÐ›Ð˜Ð•ÐÐ¢ÐžÐ’:

ceph config set osd osd_max_backfills 1
ceph config set osd osd_recovery_max_active 1
ceph config set osd osd_recovery_op_priority 1
ceph config set osd osd_recovery_sleep 0.1          # Ð¿Ð°ÑƒÐ·Ð° 100ms
ceph config set osd osd_recovery_sleep_ssd 0.05

# ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾: Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ñ‚ÑŒ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ (bytes/sec)
ceph config set osd osd_recovery_max_single_start 1
ceph config set osd osd_recovery_max_chunk 8388608  # 8MB chunks

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
ceph config dump | grep recovery

# Ð¡ÐÐ¯Ð¢Ð˜Ð• Ð¤Ð›ÐÐ“ÐžÐ’ ÐŸÐžÐ­Ð¢ÐÐŸÐÐž:

# 1. Ð¡Ð½ÑÑ‚ÑŒ norebalance ÐŸÐ•Ð Ð’Ð«Ðœ
ceph osd unset norebalance

# ÐŸÐ¾Ð´Ð¾Ð¶Ð´Ð°Ñ‚ÑŒ 5-10 Ð¼Ð¸Ð½ÑƒÑ‚, Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ
watch -n 10 'ceph -s'

# Ceph Ð¿Ð¾ÐºÐ°Ð¶ÐµÑ‚:
#   progress: Rebalancing (0.1% complete)
#   misplaced: X/Y objects (40.0%)

# 2. Ð¡Ð½ÑÑ‚ÑŒ nobackfill
ceph osd unset nobackfill

# 3. Ð¡Ð½ÑÑ‚ÑŒ noout
ceph osd unset noout

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‡Ñ‚Ð¾ Ñ„Ð»Ð°Ð³Ð¸ ÑÐ½ÑÑ‚Ñ‹
ceph osd dump | grep flags
```

#### Ð¨Ð°Ð³ 8: ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸

```bash
# ÐŸÐ¾ÑÑ‚Ð¾ÑÐ½Ð½Ñ‹Ð¹ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ (Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ð»Ðµ)
watch -n 10 'ceph -s'

# Ð’Ñ‹Ð²Ð¾Ð´ Ð±ÑƒÐ´ÐµÑ‚ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ:
#   cluster:
#     health: HEALTH_WARN
#             Degraded data redundancy: X/Y objects degraded (40.0%), ...
#   progress:
#     Rebalancing (15.2% complete)

# Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
ceph pg dump | grep -E 'active\+(remapped|backfilling)'

# ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
ceph status | grep progress

# ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ
ceph status | grep 'Rebalancing'

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð½Ð° OSD
ceph osd perf

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑƒÑ‚Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐµÑ‚Ð¸
iftop -i bond1  # cluster network

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð°Ð´ÐµÑ€Ð¶ÐµÐº ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð²
rados -p mypool bench 10 write -t 16
rados -p mypool bench 10 rand -t 16

# Ð•ÑÐ»Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ñ‹ ÑÑ‚Ñ€Ð°Ð´Ð°ÑŽÑ‚: Ð—ÐÐœÐ•Ð”Ð›Ð˜Ð¢Ð¬
ceph config set osd osd_recovery_sleep 0.2
ceph config set osd osd_max_backfills 1

# Ð•ÑÐ»Ð¸ Ð²ÑÐµ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾: ÐœÐžÐ–ÐÐž Ð£Ð¡ÐšÐžÐ Ð˜Ð¢Ð¬
ceph config set osd osd_recovery_sleep 0.05
ceph config set osd osd_max_backfills 2
```

**ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸:**

| Ð¡ÐµÑ‚ÑŒ | Ð”Ð°Ð½Ð½Ñ‹Ñ… | ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° | Ð’Ñ€ÐµÐ¼Ñ |
|------|--------|-----------|-------|
| 10 Gbit/s | 16 TB | ÐœÐµÐ´Ð»ÐµÐ½Ð½Ð°Ñ | 5-7 Ð´Ð½ÐµÐ¹ |
| 25 Gbit/s | 16 TB | ÐœÐµÐ´Ð»ÐµÐ½Ð½Ð°Ñ | 2-3 Ð´Ð½Ñ |
| 25 Gbit/s | 16 TB | Ð¡Ñ€ÐµÐ´Ð½ÑÑ | 1-2 Ð´Ð½Ñ |
| 50 Gbit/s | 16 TB | Ð¡Ñ€ÐµÐ´Ð½ÑÑ | 12-24 Ñ‡Ð°ÑÐ° |
| 100 Gbit/s | 16 TB | Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ | 6-12 Ñ‡Ð°ÑÐ¾Ð² |

#### Ð¨Ð°Ð³ 9: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ

```bash
# Ð”Ð¾Ð¶Ð´Ð°Ñ‚ÑŒÑÑ HEALTH_OK
ceph -s

# ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
#   cluster:
#     id:     abc123...
#     health: HEALTH_OK  â† ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž!
# 
#   services:
#     mon: 5 daemons, quorum node1,node2,node3,node4,node5
#     mgr: node1(active), standbys: node2,node3,node4,node5
#     osd: 50 osds: 50 up, 50 in  â† Ð±Ñ‹Ð»Ð¾ 30
#
#   data:
#     pools:   2 pools, 512 pgs
#     objects: 1.2M objects, 40 TiB
#     usage:   120 TiB used, 80 TiB / 200 TiB avail  â† Ð±Ð¾Ð»ÑŒÑˆÐµ Ð¼ÐµÑÑ‚Ð°
#     pgs:     512 active+clean  â† Ð²ÑÐµ Ð·Ð´Ð¾Ñ€Ð¾Ð²Ñ‹!

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…
ceph osd df tree

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð²Ð°Ñ€Ð¸Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
ceph osd df tree | grep VAR
# VAR Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ Ð±Ð»Ð¸Ð·ÐºÐ° Ðº 1.0 (Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ð¾ 0.9-1.1)

# Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ
ceph health detail
# ÐÐµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ Ð½Ð¸ÐºÐ°ÐºÐ¸Ñ… warning!

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° PG
ceph pg stat
ceph pg dump | grep -v active+clean
# ÐÐµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ Ð²Ñ‹Ð²Ð¾Ð´Ð° (Ð²ÑÐµ PG active+clean)

# Ð¢ÐµÑÑ‚ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸
rados -p mypool bench 30 write --no-cleanup
rados -p mypool bench 30 seq
rados -p mypool bench 30 rand

# Ð¡Ð±Ñ€Ð¾Ñ Ð¾Ñ‡ÐµÑ€ÐµÐ´Ð¸ ÐºÐ¾Ð¼Ð°Ð½Ð´
rados -p mypool cleanup
```

### Ð­Ñ‚Ð°Ð¿ 2: Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ 5 â†’ 7 ÑƒÐ·Ð»Ð¾Ð²

ÐŸÑ€Ð¾Ñ†ÐµÑÑ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ñ‹Ð¹, Ð½Ð¾ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¼ÐµÐ½ÑŒÑˆÐµ:

```
Ð‘Ñ‹Ð»Ð¾: 50 OSD
Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼: 20 OSD (2 ÑƒÐ·Ð»Ð° Ã— 10 OSD)
Ð¡Ñ‚Ð°Ð½ÐµÑ‚: 70 OSD

ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸ = (20 / 70) Ã— 100% â‰ˆ 28.5% Ð´Ð°Ð½Ð½Ñ‹Ñ…

ÐŸÑ€Ð¸ 40TB Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿ÐµÑ€ÐµÐ¼ÐµÑÑ‚Ð¸Ñ‚ÑÑ ~11.4TB
Ð’Ñ€ÐµÐ¼Ñ Ð¿Ñ€Ð¸ 25 Gbit/s: ~1-2 Ð´Ð½Ñ
```

**ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð²ÑÐµ ÑˆÐ°Ð³Ð¸ Ð¸Ð· Ð­Ñ‚Ð°Ð¿Ð° 1:**
1. ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° node6 Ð¸ node7
2. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ñ„Ð»Ð°Ð³Ð¾Ð²
3. Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð² ÐºÐ»Ð°ÑÑ‚ÐµÑ€
4. ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ CRUSH Map
5. Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ MON (Ñ‚ÐµÐ¿ÐµÑ€ÑŒ 7 MON)
6. Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ OSD
7. ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð°Ñ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ°
8. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°

### Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑƒÐ·Ð»Ð¾Ð²

#### Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ 1: ÐŸÐ¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ð¾Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ (Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð¸ÑÐº)

```bash
# Ð”ÐµÐ½ÑŒ 1: Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ node4
ceph osd set noout norebalance nobackfill
# ... Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ node4 ...
ceph osd unset norebalance nobackfill noout
# Ð–Ð´Ð°Ñ‚ÑŒ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸ (3-5 Ð´Ð½ÐµÐ¹)

# Ð”ÐµÐ½ÑŒ 6: Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ node5
ceph osd set noout norebalance nobackfill
# ... Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ node5 ...
ceph osd unset norebalance nobackfill noout
# Ð–Ð´Ð°Ñ‚ÑŒ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ

# ÐŸÐ»ÑŽÑÑ‹:
# - ÐœÐµÐ½ÑŒÑˆÐµ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
# - ÐŸÑ€Ð¾Ñ‰Ðµ Ð¾Ñ‚ÐºÐ°Ñ‚Ð¸Ñ‚ÑŒÑÑ Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ…
# - Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½ÐµÐµ

# ÐœÐ¸Ð½ÑƒÑÑ‹:
# - ÐžÑ‡ÐµÐ½ÑŒ Ð´Ð¾Ð»Ð³Ð¾ (ÑƒÐ´Ð²Ð¾ÐµÐ½Ð½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ)
# - Ð‘Ð¾Ð»ÑŒÑˆÐµ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²Ð¾Ðº (2 Ñ€Ð°Ð·Ð°)
```

#### Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ 2: ÐÐ¾Ñ‡Ð½Ð°Ñ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° (Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ)

```bash
# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÑƒÐ·Ð»Ñ‹ Ñ Ñ„Ð»Ð°Ð³Ð°Ð¼Ð¸
ceph osd set noout norebalance nobackfill
# ... Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ node4 Ð¸ node5 ...

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÑƒ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð¾Ñ‡ÑŒÑŽ
ceph config set osd osd_recovery_begin_hour 22  # 22:00
ceph config set osd osd_recovery_end_hour 6     # 06:00

# Ð¡Ð½ÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸
ceph osd unset norebalance nobackfill noout

# ÐŸÐ»ÑŽÑÑ‹:
# - ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð½Ð° ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð½ÐµÐ¼
# - ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÐ¼Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹

# ÐœÐ¸Ð½ÑƒÑÑ‹:
# - ÐžÐ§Ð•ÐÐ¬ Ð´Ð¾Ð»Ð³Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ (Ð½ÐµÐ´ÐµÐ»Ð¸)
# - Ð¢Ð¾Ð»ÑŒÐºÐ¾ 8 Ñ‡Ð°ÑÐ¾Ð² Ð² ÑÑƒÑ‚ÐºÐ¸ Ð´Ð»Ñ recovery
```

#### Ð¡Ñ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ 3: Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ñ€ÐµÐ±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ° Ð² Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ

```bash
# Ð—Ð°Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð½Ð° Ð¿ÑÑ‚Ð½Ð¸Ñ†Ñƒ Ð²ÐµÑ‡ÐµÑ€

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð°Ð³Ñ€ÐµÑÑÐ¸Ð²Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
ceph config set osd osd_max_backfills 4
ceph config set osd osd_recovery_max_active 3
ceph config set osd osd_recovery_sleep 0
ceph config set osd osd_recovery_sleep_ssd 0

# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÑƒÐ·Ð»Ñ‹ Ð¸ ÑÐ½ÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸
# ...

# ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð² Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ
# ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ: 24-48 Ñ‡Ð°ÑÐ¾Ð²

# ÐŸÐ¾Ð½ÐµÐ´ÐµÐ»ÑŒÐ½Ð¸Ðº: Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
ceph config set osd osd_max_backfills 1
ceph config set osd osd_recovery_max_active 1
ceph config set osd osd_recovery_sleep 0.1

# ÐŸÐ»ÑŽÑÑ‹:
# - Ð‘Ñ‹ÑÑ‚Ñ€Ð¾ (1-2 Ð´Ð½Ñ)
# - ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ð»Ð¸ÑÐ½Ð¸Ðµ Ð½Ð° production (Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ)

# ÐœÐ¸Ð½ÑƒÑÑ‹:
# - ÐÑƒÐ¶Ð½Ð¾ Ð´ÐµÐ¶ÑƒÑ€Ð¸Ñ‚ÑŒ Ð² Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ
# - Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð½Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€
```

### Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ·Ð»Ð¾Ð² Ð¸Ð· ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°

âš ï¸ **Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ñ Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒÑŽ!**

```bash
# Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹: Ð²Ñ‹Ð²Ð¾Ð´ node5 Ð¸Ð· ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°

# 1. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸
ceph osd set noout norebalance nobackfill

# 2. ÐŸÐ¾Ð¼ÐµÑ‚Ð¸Ñ‚ÑŒ Ð²ÑÐµ OSD ÑƒÐ·Ð»Ð° ÐºÐ°Ðº out
for osd_id in $(ceph osd ls-tree node5); do
  ceph osd out osd.$osd_id
done

# 3. Ð¡Ð½ÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³ norebalance (Ð½Ð°Ñ‡Ð½ÐµÑ‚ÑÑ Ð¿ÐµÑ€ÐµÐ¼ÐµÑ‰ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…)
ceph osd unset norebalance

# 4. Ð”Ð¾Ð¶Ð´Ð°Ñ‚ÑŒÑÑ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ (Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐµÐ´ÑƒÑ‚ Ð½Ð° Ð´Ñ€ÑƒÐ³Ð¸Ðµ OSD)
watch -n 10 'ceph -s'
# Ð–Ð´Ð°Ñ‚ÑŒ HEALTH_OK

# 5. ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ OSD Ð½Ð° node5
ssh root@node5 'systemctl stop ceph-osd.target'

# 6. Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ OSD Ð¸Ð· ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
for osd_id in $(ceph osd ls-tree node5); do
  ceph osd purge osd.$osd_id --yes-i-really-mean-it
done

# 7. Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑƒÐ·ÐµÐ» Ð¸Ð· CRUSH Map
ceph osd crush remove node5

# 8. Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑƒÐ·ÐµÐ» Ð¸Ð· ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
ceph orch host rm node5 --force

# 9. Ð¡Ð½ÑÑ‚ÑŒ Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸ÐµÑÑ Ñ„Ð»Ð°Ð³Ð¸
ceph osd unset noout nobackfill

# 10. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph osd tree
ceph -s
```

---

## ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¸ Ð°Ð»ÐµÑ€Ñ‚Ð¸Ð½Ð³

### Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ceph Dashboard

```bash
# Dashboard Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð¿Ð¾ÑÐ»Ðµ bootstrap
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph mgr module ls | grep dashboard

# Ð•ÑÐ»Ð¸ Ð²Ñ‹ÐºÐ»ÑŽÑ‡ÐµÐ½
ceph mgr module enable dashboard

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
ceph dashboard ac-user-create admin <strong-password> administrator

# Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ SSL (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶ÐµÐ½ ÐºÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ ÑÐµÑ€Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚)
ceph dashboard create-self-signed-cert

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð°Ð´Ñ€ÐµÑÐ°
ceph config set mgr mgr/dashboard/server_addr 0.0.0.0
ceph config set mgr mgr/dashboard/server_port 8443

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° URL
ceph mgr services

# Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
# {
#     "dashboard": "https://node1:8443/"
# }

# Ð”Ð¾ÑÑ‚ÑƒÐ¿: https://node1:8443
# Ð›Ð¾Ð³Ð¸Ð½: admin
# ÐŸÐ°Ñ€Ð¾Ð»ÑŒ: <strong-password>
```

### ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Prometheus Ð¸ Grafana

```bash
# Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Prometheus
ceph mgr module enable prometheus

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° ÑÐ½Ð´Ð¿Ð¾Ð¸Ð½Ñ‚Ð°
ceph config set mgr mgr/prometheus/server_addr 0.0.0.0
ceph config set mgr mgr/prometheus/server_port 9283

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¼ÐµÑ‚Ñ€Ð¸Ðº
curl http://localhost:9283/metrics

# Ð Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚ÐµÐºÐ° Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð° Ñ‡ÐµÑ€ÐµÐ· cephadm
ceph orch apply grafana --placement="node1"
ceph orch apply prometheus --placement="node1"
ceph orch apply alertmanager --placement="node1"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²
ceph orch ps | grep -E 'grafana|prometheus|alertmanager'

# URL Grafana (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ)
# http://node1:3000
# Ð›Ð¾Ð³Ð¸Ð½: admin
# ÐŸÐ°Ñ€Ð¾Ð»ÑŒ: admin (Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð²Ñ…Ð¾Ð´Ðµ)
```

**Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Prometheus Ñ Ð²Ð½ÐµÑˆÐ½Ð¸Ð¼ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ð¾Ð¼:**

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'ceph'
    static_configs:
      - targets: ['node1:9283', 'node2:9283', 'node3:9283']
```

### ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð°Ð»ÐµÑ€Ñ‚Ð¾Ð²

**Alertmanager Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð° Ð´Ð»Ñ Ceph:**

```yaml
# ceph-alerts.yml
groups:
  - name: ceph
    interval: 30s
    rules:
      # Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
      - alert: CephHealthWarning
        expr: ceph_health_status == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Ceph cluster health WARNING"
          description: "Cluster {{ $labels.cluster }} has status HEALTH_WARN"

      - alert: CephHealthError
        expr: ceph_health_status == 2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Ceph cluster health ERROR"
          description: "Cluster {{ $labels.cluster }} has status HEALTH_ERR"

      # OSD ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
      - alert: CephOSDDown
        expr: ceph_osd_up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Ceph OSD Down"
          description: "OSD {{ $labels.ceph_daemon }} is down for more than 5 minutes"

      - alert: CephOSDNearFull
        expr: ceph_osd_utilization > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Ceph OSD near full"
          description: "OSD {{ $labels.ceph_daemon }} is {{ $value }}% full"

      - alert: CephOSDFull
        expr: ceph_osd_utilization > 90
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Ceph OSD critically full"
          description: "OSD {{ $labels.ceph_daemon }} is {{ $value }}% full"

      # Ð—Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
      - alert: CephClusterNearFull
        expr: ceph_cluster_total_used_bytes / ceph_cluster_total_bytes > 0.75
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Ceph cluster near full"
          description: "Cluster is {{ $value | humanizePercentage }} full"

      # Degraded objects
      - alert: CephDegradedObjects
        expr: ceph_pg_degraded > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Ceph has degraded objects"
          description: "{{ $value }} objects are degraded"

      # PG ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
      - alert: CephPGsNotActive
        expr: ceph_pg_total - ceph_pg_active > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Some PGs are not active"
          description: "{{ $value }} PGs are not in active state"

      # MON ÐºÐ²Ð¾Ñ€ÑƒÐ¼
      - alert: CephMonQuorumLost
        expr: ceph_mon_quorum_count < ((ceph_mon_count / 2) + 1)
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Ceph MON quorum lost"
          description: "Only {{ $value }} monitors in quorum"
```

### ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ SMART Ð´Ð¸ÑÐºÐ¾Ð²

```bash
# Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ devicehealth
ceph mgr module enable devicehealth

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°
ceph config set mgr mgr/devicehealth/enable_monitoring true
ceph config set mgr mgr/devicehealth/scrape_frequency 86400  # Ñ€Ð°Ð· Ð² Ð´ÐµÐ½ÑŒ

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° SMART Ð¼ÐµÑ‚Ñ€Ð¸Ðº
ceph device ls
ceph device info <device-id>
ceph device query-daemon-health-metrics <daemon>

# ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹
ceph device get-health-metrics <device-id>

# ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ñ‹Ðµ Ð¿Ñ€ÐµÐ´Ð¸ÐºÑ‚Ð¾Ñ€Ñ‹ Ð¾Ñ‚ÐºÐ°Ð·Ð°:
# - Reallocated_Sector_Ct > 0
# - Current_Pending_Sector > 0
# - Offline_Uncorrectable > 0
# - UDMA_CRC_Error_Count > 1000
```

**Ð ÑƒÑ‡Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° SMART:**

```bash
# ÐÐ° ÐºÐ°Ð¶Ð´Ð¾Ð¼ ÑƒÐ·Ð»Ðµ
for disk in /dev/sd{a..z}; do
  [ -b "$disk" ] && smartctl -a $disk | grep -E 'Model|Serial|Reallocated|Pending|Uncorrectable|Temperature'
done

# Ð”Ð»Ñ NVMe
for disk in /dev/nvme*n1; do
  [ -b "$disk" ] && nvme smart-log $disk | grep -E 'temperature|available_spare|percentage_used'
done
```

### Ð¡ÐºÑ€Ð¸Ð¿Ñ‚Ñ‹ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°

**Ð•Ð¶ÐµÐ´Ð½ÐµÐ²Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ:**

```bash
#!/bin/bash
# /usr/local/bin/ceph-daily-check.sh

MAILTO="admin@example.com"
SUBJECT="Ceph Daily Health Check"

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ
HEALTH=$(ceph health detail)

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
USAGE=$(ceph df | grep -A 5 "GLOBAL")

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° OSD
OSD_STAT=$(ceph osd stat)
OSD_DOWN=$(ceph osd tree | grep down | wc -l)

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° PG
PG_STAT=$(ceph pg stat)
PG_NOT_CLEAN=$(ceph pg dump | grep -v active+clean | wc -l)

# Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
REPORT="Ceph Cluster Health Report
========================

Health Status:
$HEALTH

Usage:
$USAGE

OSD Status:
$OSD_STAT
OSDs Down: $OSD_DOWN

PG Status:
$PG_STAT
PGs not clean: $PG_NOT_CLEAN
"

# ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° email
echo "$REPORT" | mail -s "$SUBJECT" $MAILTO

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Ð»Ð¾Ð³
echo "$REPORT" >> /var/log/ceph-daily-check.log
```

```bash
# Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð² cron
cat > /etc/cron.d/ceph-daily-check <<EOF
0 8 * * * root /usr/local/bin/ceph-daily-check.sh
EOF
```

---

## Ð­ÐºÑÐ¿Ð»ÑƒÐ°Ñ‚Ð°Ñ†Ð¸Ñ Ð¸ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ

### Ð—Ð°Ð¼ÐµÐ½Ð° Ð½ÐµÐ¸ÑÐ¿Ñ€Ð°Ð²Ð½Ð¾Ð³Ð¾ Ð´Ð¸ÑÐºÐ°

**Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹: OSD.5 Ð½Ð° node2 Ð²Ñ‹ÑˆÐµÐ» Ð¸Ð· ÑÑ‚Ñ€Ð¾Ñ**

```bash
# Ð¨Ð°Ð³ 1: Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°
ceph osd tree | grep down
# 5   ssd    3.00000         osd.5        down   1.00000  1.00000

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð»Ð¾Ð³Ð¾Ð²
journalctl -u ceph-osd@5 -n 100

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° SMART (ÐµÑÐ»Ð¸ Ð´Ð¸ÑÐº Ð²Ð¸Ð´ÐµÐ½)
smartctl -a /dev/sdc

# Ð¨Ð°Ð³ 2: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸ (ÐµÑÐ»Ð¸ Ð·Ð°Ð¼ÐµÐ½ÑÐµÑ‚Ðµ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ < 1 Ñ‡Ð°ÑÐ°)
ceph osd set noout
# Ð­Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‚Ð¸Ñ‚ Ð½Ð°Ñ‡Ð°Ð»Ð¾ recovery

# Ð¨Ð°Ð³ 3: ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ OSD (ÐµÑÐ»Ð¸ ÐµÑ‰Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚)
systemctl stop ceph-osd@5

# Ð¨Ð°Ð³ 4: Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ OSD Ð¸Ð· ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
ceph osd out osd.5
ceph osd purge osd.5 --yes-i-really-mean-it

# Ð¨Ð°Ð³ 5: Ð¤Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð·Ð°Ð¼ÐµÐ½Ð° Ð´Ð¸ÑÐºÐ°
# - Ð’Ñ‹ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²ÐµÑ€ (Ð¸Ð»Ð¸ hot-swap ÐµÑÐ»Ð¸ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ)
# - Ð—Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð´Ð¸ÑÐº
# - Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²ÐµÑ€

# Ð¨Ð°Ð³ 6: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð´Ð¸ÑÐºÐ°
lsblk
smartctl -a /dev/sdc

# Ð¨Ð°Ð³ 7: ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð¸ÑÐºÐ° (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾)
wipefs -a /dev/sdc
dd if=/dev/zero of=/dev/sdc bs=1M count=100

# Ð¨Ð°Ð³ 8: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð½Ð¾Ð²Ð¾Ð³Ð¾ OSD
ceph orch daemon add osd node2:/dev/sdc

# Ð˜Ð›Ð˜ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ (ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´)
ceph-volume lvm create --data /dev/sdc

# Ð¨Ð°Ð³ 9: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph osd tree
# ÐÐ¾Ð²Ñ‹Ð¹ OSD Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¿Ð¾ÑÐ²Ð¸Ñ‚ÑŒÑÑ (Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ ID)

# Ð¨Ð°Ð³ 10: Ð¡Ð½ÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³ (Ð½Ð°Ñ‡Ð½ÐµÑ‚ÑÑ recovery)
ceph osd unset noout

# Ð¨Ð°Ð³ 11: ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ recovery
watch -n 10 'ceph -s'
# Ð–Ð´Ð°Ñ‚ÑŒ HEALTH_OK
```

### Ð—Ð°Ð¼ÐµÐ½Ð° Ñ†ÐµÐ»Ð¾Ð³Ð¾ ÑƒÐ·Ð»Ð°

```bash
# Ð¡Ñ†ÐµÐ½Ð°Ñ€Ð¸Ð¹: node3 Ð²Ñ‹ÑˆÐµÐ» Ð¸Ð· ÑÑ‚Ñ€Ð¾Ñ, Ð½ÑƒÐ¶Ð½Ð¾ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð²ÐµÑÑŒ ÑÐµÑ€Ð²ÐµÑ€

# Ð¨Ð°Ð³ 1: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸
ceph osd set noout norecover nobackfill norebalance

# Ð¨Ð°Ð³ 2: ÐŸÐ¾Ð¼ÐµÑ‚Ð¸Ñ‚ÑŒ Ð²ÑÐµ OSD ÑƒÐ·Ð»Ð° ÐºÐ°Ðº out
for osd_id in $(ceph osd ls-tree node3); do
  ceph osd out osd.$osd_id
done

# Ð¨Ð°Ð³ 3: Ð•ÑÐ»Ð¸ ÑƒÐ·ÐµÐ» Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ - Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð´ÐµÐ¼Ð¾Ð½Ñ‹
ssh node3 'systemctl stop ceph.target'

# Ð¨Ð°Ð³ 4: Ð¤Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð·Ð°Ð¼ÐµÐ½Ð° ÑÐµÑ€Ð²ÐµÑ€Ð°
# - Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð½Ð¾Ð²Ð¾Ð³Ð¾ ÑÐµÑ€Ð²ÐµÑ€Ð°
# - Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÐžÐ¡
# - ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº

# Ð¨Ð°Ð³ 5: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ·Ð»Ð° Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ (Ñ Ñ‚ÐµÐ¼ Ð¶Ðµ Ð¸Ð¼ÐµÐ½ÐµÐ¼)
ssh-copy-id -f -i /etc/ceph/ceph.pub root@node3
ceph orch host add node3 10.0.1.13 --labels _admin

# Ð¨Ð°Ð³ 6: ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ CRUSH
ceph osd crush move node3 rack=rack3

# Ð¨Ð°Ð³ 7: Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ OSD
ceph orch apply osd --all-available-devices --hostname=node3

# Ð¨Ð°Ð³ 8: Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ñ‹Ñ… OSD (ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð¸)
# ÐŸÐ¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ ÑÑ‚Ð°Ñ€Ñ‹Ðµ OSD Ñ node3
ceph osd tree | grep node3 | grep down
# Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÐºÐ°Ð¶Ð´Ñ‹Ð¹
ceph osd purge osd.X --yes-i-really-mean-it

# Ð¨Ð°Ð³ 9: Ð¡Ð½ÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸
ceph osd unset noout norecover nobackfill norebalance

# Ð¨Ð°Ð³ 10: ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³
watch -n 10 'ceph -s'
```

### ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ceph

**âš ï¸ ÐšÐ Ð˜Ð¢Ð˜Ð§ÐÐž: Ð’ÑÐµÐ³Ð´Ð° Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Release Notes!**

```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð²ÐµÑ€ÑÐ¸Ð¸
ceph versions

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð²ÐµÑ€ÑÐ¸Ð¹ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
ceph orch upgrade ls

# ÐŸÐ•Ð Ð•Ð” ÐžÐ‘ÐÐžÐ’Ð›Ð•ÐÐ˜Ð•Ðœ:
# 1. Ð¡Ð´ÐµÐ»Ð°Ñ‚ÑŒ backup ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸
mkdir -p /root/ceph-backup-$(date +%F)
ceph config dump > /root/ceph-backup-$(date +%F)/config.txt
ceph osd crush dump > /root/ceph-backup-$(date +%F)/crush-map.json
ceph auth export > /root/ceph-backup-$(date +%F)/auth-keys.txt

# 2. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÐµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
ceph health detail
# Ð”Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ HEALTH_OK!

# 3. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾, Ð´Ð»Ñ safety)
ceph osd set noout
ceph osd set noscrub
ceph osd set nodeep-scrub

# Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
ceph orch upgrade start --image quay.io/ceph/ceph:v18.2.1

# ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
ceph orch upgrade status

# Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚:
# {
#     "target_image": "quay.io/ceph/ceph:v18.2.1",
#     "in_progress": true,
#     "which": "Upgrading mgr.node1",
#     "services_complete": ["mon", "crash"],
#     "progress": "3/12 daemons upgraded",
#     "message": ""
# }

# ÐŸÐ¾ÑÑ‚Ð¾ÑÐ½Ð½Ñ‹Ð¹ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³
watch -n 30 'ceph orch upgrade status; ceph -s'

# ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð´ÐµÑ‚ Ð² Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ:
# 1. MON Ð´ÐµÐ¼ Ð¾Ð½Ñ‹
# 2. MGR Ð´ÐµÐ¼Ð¾Ð½Ñ‹
# 3. OSD Ð´ÐµÐ¼Ð¾Ð½Ñ‹ (Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ñ…Ð¾ÑÑ‚Ð°)
# 4. MDS Ð´ÐµÐ¼Ð¾Ð½Ñ‹ (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
# 5. RGW Ð´ÐµÐ¼Ð¾Ð½Ñ‹ (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)

# ÐŸÐ¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ (Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ‡Ð°ÑÐ¾Ð²)
ceph versions
# Ð’ÑÐµ Ð´ÐµÐ¼Ð¾Ð½Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ð½Ð° Ð½Ð¾Ð²Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸

# Ð¡Ð½ÑÑ‚ÑŒ Ñ„Ð»Ð°Ð³Ð¸
ceph osd unset noout noscrub nodeep-scrub

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°
ceph health detail
```

**ÐžÑ‚ÐºÐ°Ñ‚ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ (ÐµÑÐ»Ð¸ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð¿Ð¾ÑˆÐ»Ð¾ Ð½Ðµ Ñ‚Ð°Ðº):**

```bash
# ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ
ceph orch upgrade stop

# ÐžÑ‚ÐºÐ°Ñ‚Ð¸Ñ‚ÑŒÑÑ Ð½Ð° Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÑƒÑŽ Ð²ÐµÑ€ÑÐ¸ÑŽ
ceph orch upgrade start --image quay.io/ceph/ceph:v18.2.0

# ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð¾Ñ‚ÐºÐ°Ñ‚Ð°
ceph orch upgrade status
```

### Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾Ðµ Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ

**Ð•Ð¶ÐµÐ½ÐµÐ´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸:**

```bash
#!/bin/bash
# /usr/local/bin/ceph-weekly-maintenance.sh

# 1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° SMART Ð´Ð¸ÑÐºÐ¾Ð²
echo "=== SMART Check ==="
ceph device ls
ceph device query-daemon-health-metrics

# 2. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð±Ð°Ð»Ð°Ð½ÑÐ¸Ñ€Ð¾Ð²ÐºÐ¸
echo "=== OSD Balance Check ==="
ceph osd df tree | grep VAR
# VAR Ð´Ð¾Ð»Ð¶Ð½Ð° Ð±Ñ‹Ñ‚ÑŒ 0.9-1.1

# 3. ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ scrub (ÐµÑÐ»Ð¸ Ð´Ð°Ð²Ð½Ð¾ Ð½Ðµ Ð±Ñ‹Ð»Ð¾)
echo "=== Initiating scrub ==="
for pool in $(ceph osd pool ls); do
  ceph osd pool scrub $pool
done

# 4. ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° ÑÑ‚Ð°Ñ€Ñ‹Ñ… ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð¾Ð² (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ)
echo "=== Snapshot cleanup ==="
# rbd snap ls <pool>/<image>
# rbd snap purge <pool>/<image>

# 5. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð»Ð¾Ð³Ð¾Ð² Ð½Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ¸
echo "=== Error log check ==="
journalctl -u 'ceph-*' --since "7 days ago" | grep -i error

# 6. Backup ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸
echo "=== Config backup ==="
BACKUP_DIR="/root/ceph-backup-weekly-$(date +%F)"
mkdir -p $BACKUP_DIR
ceph config dump > $BACKUP_DIR/config.txt
ceph osd crush dump > $BACKUP_DIR/crush-map.json
ceph auth export > $BACKUP_DIR/auth-keys.txt
```

**Ð•Ð¶ÐµÐ¼ÐµÑÑÑ‡Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸:**

- ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚Ð¸ (rados bench)
- ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚Ñ€ÐµÐ½Ð´Ð¾Ð² Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
- ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ð´Ð¸ÑÐºÐ¾Ð² (> 3 Ð»ÐµÑ‚)
- Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ backup/restore
- ÐžÐ±Ð·Ð¾Ñ€ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸ (CVE, Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ)

### Disaster Recovery

**Backup critical data:**

```bash
# 1. ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ MON
ceph mon dump > mon-dump-$(date +%F).txt

# 2. ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ OSD
ceph osd dump > osd-dump-$(date +%F).txt

# 3. CRUSH Map
ceph osd getcrushmap -o crushmap-$(date +%F).bin
crushtool -d crushmap-$(date +%F).bin -o crushmap-$(date +%F).txt

# 4. ÐšÐ»ÑŽÑ‡Ð¸ Ð°Ð²Ñ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
ceph auth list > auth-list-$(date +%F).txt

# 5. ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¿ÑƒÐ»Ð¾Ð²
for pool in $(ceph osd pool ls); do
  ceph osd pool get $pool all > pool-$pool-$(date +%F).txt
done

# 6. RBD Ð¾Ð±Ñ€Ð°Ð·Ñ‹ (ÑÐ¿Ð¸ÑÐ¾Ðº)
for pool in $(ceph osd pool ls | grep rbd); do
  rbd ls $pool > rbd-list-$pool-$(date +%F).txt
done
```

**Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ MON ÐºÐ²Ð¾Ñ€ÑƒÐ¼Ð° (ÐµÑÐ»Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑÐ»Ð¸ > 50% MON):**

```bash
# ÐšÐ ÐÐ™ÐÐ˜Ð™ Ð¡Ð›Ð£Ð§ÐÐ™! Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð¿Ð¾Ñ‚ÐµÑ€ÑÐ»Ð¸ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð½ÑÑ‚Ð²Ð¾ MON!

# ÐÐ° surviving MON:
systemctl stop ceph-mon.target

# Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð· monmap
ceph-mon -i <mon-id> --extract-monmap /tmp/monmap

# Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ failed MON Ð¸Ð· monmap
monmaptool /tmp/monmap --rm <failed-mon-id>

# Inject monmap Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾
ceph-mon -i <mon-id> --inject-monmap /tmp/monmap

# Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ MON
systemctl start ceph-mon.target


----â€”--------------
-------------------

# Ceph Cluster - Troubleshooting Guide & Checklist

## Troubleshooting

### 1. Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð¾Ð±Ñ‰ÐµÐ³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°

#### ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°: HEALTH_WARN Ð¸Ð»Ð¸ HEALTH_ERR
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ
ceph health detail
ceph status

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸
kubectl logs -n rook-ceph deploy/rook-ceph-operator
kubectl logs -n rook-ceph -l app=rook-ceph-mon
```

**Ð Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ñ‹:**
- OSD down/out
- ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð¼ÐµÑÑ‚Ð° Ð½Ð° Ð´Ð¸ÑÐºÐ°Ñ…
- Clock skew Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¾Ð´Ð°Ð¼Ð¸
- ÐœÐµÐ´Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸ (slow ops)

---

### 2. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ OSD

#### OSD Ð½Ðµ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÑŽÑ‚ÑÑ
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ OSD
ceph osd tree
ceph osd stat

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ OSD
kubectl logs -n rook-ceph -l app=rook-ceph-osd -c osd

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ deployments OSD
kubectl get deployment -n rook-ceph -l app=rook-ceph-osd
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð¸ÑÐºÐ¾Ð² Ð½Ð° Ð½Ð¾Ð´Ð°Ñ…
- Ð£Ð±ÐµÐ´Ð¸Ñ‚ÑŒÑÑ Ñ‡Ñ‚Ð¾ Ð´Ð¸ÑÐºÐ¸ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ°Ð¼Ð¸: `lsblk`
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº Ð´Ð¸ÑÐºÐ°Ð¼
- ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ failed OSD: 
  ```bash
  ceph osd purge <osd-id> --yes-i-really-mean-it
  ```

#### OSD full Ð¸Ð»Ð¸ nearfull
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ
ceph osd df tree

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ pool ÐºÐ²Ð¾Ñ‚Ñ‹
ceph osd pool get-quota <pool-name>
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ storage capacity (Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ð¸ÑÐºÐ¸/Ð½Ð¾Ð´Ñ‹)
- Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ€Ñ‹Ðµ snapshots
- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÑƒ: 
  ```bash
  ceph osd set-nearfull-ratio 0.85
  ceph osd set-full-ratio 0.95
  ```

#### ÐœÐµÐ´Ð»ÐµÐ½Ð½Ñ‹Ðµ OSD (slow ops)
```bash
# ÐÐ°Ð¹Ñ‚Ð¸ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸Ð¸
ceph health detail | grep slow

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ latency Ð´Ð¸ÑÐºÐ¾Ð²
ceph osd perf
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ CPU/Memory Ð½Ð° Ð½Ð¾Ð´Ð°Ñ…
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ disk I/O: `iostat -x 1`
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ network latency Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¾Ð´Ð°Ð¼Ð¸
- Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹: 
  ```bash
  ceph config set osd osd_op_complaint_time 60
  ```

---

### 3. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ MON

#### MON quorum Ð¿Ð¾Ñ‚ÐµÑ€ÑÐ½
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ MON
ceph mon stat
kubectl get pod -n rook-ceph -l app=rook-ceph-mon

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ connectivity Ð¼ÐµÐ¶Ð´Ñƒ MON
kubectl exec -n rook-ceph <mon-pod> -- ceph daemon mon.<id> mon_status
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- Ð£Ð±ÐµÐ´Ð¸Ñ‚ÑŒÑÑ Ñ‡Ñ‚Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 MON Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ñ‹
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ð°ÑÐ¾Ð²Ñ‹Ðµ Ð¿Ð¾ÑÑÐ° Ð½Ð° Ð½Ð¾Ð´Ð°Ñ…: `timedatectl`
- Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ NTP ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸ÑŽ
- Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ MON Ð¸Ð· backup ÐµÑÐ»Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾

#### Clock skew
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ñƒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
ceph health detail | grep clock
```

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# ÐÐ° ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð½Ð¾Ð´Ðµ
sudo timedatectl set-ntp true
sudo systemctl restart systemd-timesyncd

# Ð’ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ðµ
ceph config set global mon_clock_drift_allowed 0.05
```

---

### 4. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ PG (Placement Groups)

#### PG stuck in activating/peering
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ PG
ceph pg stat
ceph pg dump | grep -E 'stale|down|inconsistent|incomplete'

# Ð”ÐµÑ‚Ð°Ð»Ð¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ PG
ceph pg <pg-id> query
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- Ð”Ð¾Ð¶Ð´Ð°Ñ‚ÑŒÑÑ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ recovery
- Ð•ÑÐ»Ð¸ PG Ð·Ð°ÑÑ‚Ñ€ÑÐ»Ð° Ð½Ð°Ð´Ð¾Ð»Ð³Ð¾:
  ```bash
  ceph pg repair <pg-id>
  ```
- Ð’ ÐºÑ€Ð°Ð¹Ð½ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ðµ:
  ```bash
  ceph pg force-recovery <pg-id>
  ceph pg force-backfill <pg-id>
  ```

#### Undersized/degraded PGs
```bash
ceph health detail | grep pg
ceph pg ls undersized
ceph pg ls degraded
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- ÐžÐ±Ñ‹Ñ‡Ð½Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° Ñ€ÐµÑˆÐ°ÐµÑ‚ÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð¿Ð¾ÑÐ»Ðµ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ OSD
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ OSD Ð´Ð»Ñ replicas
- Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ¸Ñ‚ÑŒ replication size (Ð½Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ prod):
  ```bash
  ceph osd pool set <pool-name> min_size 1
  ```

#### Inconsistent PGs
```bash
# ÐÐ°Ð¹Ñ‚Ð¸ inconsistent PG
ceph health detail | grep inconsistent

# Ð”ÐµÑ‚Ð°Ð»Ð¸
ceph pg <pg-id> list_unfound
```

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# ÐŸÐ¾Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ
ceph pg repair <pg-id>

# Ð•ÑÐ»Ð¸ Ð½Ðµ Ð¿Ð¾Ð¼Ð¾Ð³Ð»Ð¾ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ð¾!)
ceph pg <pg-id> mark_unfound_lost revert
```

---

### 5. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ RBD (Block Storage)

#### PVC Ð½Ðµ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ StorageClass
kubectl get sc
kubectl describe sc <storage-class-name>

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ RBD provisioner
kubectl logs -n rook-ceph -l app=csi-rbdplugin-provisioner

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ PVC
kubectl describe pvc <pvc-name>
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¿ÑƒÐ»Ð°: `ceph osd lspool`
- Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¿ÑƒÐ» ÐµÑÐ»Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ CSI pods: `kubectl get pods -n rook-ceph | grep csi`

#### RBD image Ð½Ðµ Ð¼Ð¾Ð½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ðº Ð¿Ð¾Ð´Ñƒ
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ CSI nodeplugin
kubectl logs -n rook-ceph -l app=csi-rbdplugin -c csi-rbdplugin

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ RBD Ð½Ð° Ð½Ð¾Ð´Ðµ
rbd ls <pool-name>
rbd info <pool-name>/<image-name>
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ kernel modules: `lsmod | grep rbd`
- Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ: `sudo modprobe rbd`
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ network connectivity Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¾Ð´Ð¾Ð¹ Ð¸ Ceph cluster

#### RBD image Ð² exclusive-lock ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ locks
rbd lock list <pool-name>/<image-name>
```

**Ð ÐµÑˆÐµÐ½Ð¸Ðµ:**
```bash
# Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ lock (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð·Ð½Ð°ÐµÑ‚Ðµ Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð´ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ image)
rbd lock remove <pool-name>/<image-name> <lock-id> <locker>
```

---

### 6. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ CephFS

#### MDS Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚ÑƒÑ MDS
ceph fs status
ceph mds stat

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ MDS pods
kubectl get pods -n rook-ceph -l app=rook-ceph-mds
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- Ð£Ð±ÐµÐ´Ð¸Ñ‚ÑŒÑÑ Ñ‡Ñ‚Ð¾ MDS pods Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ñ‹
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸: `kubectl logs -n rook-ceph -l app=rook-ceph-mds`
- Restart MDS ÐµÑÐ»Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾

#### CephFS Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð¾Ð²
ceph fs status
ceph fs perf stats

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ cache
ceph daemon mds.<id> cache status
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ MDS cache: 
  ```bash
  ceph config set mds mds_cache_memory_limit 4294967296
  ```
- Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ active MDS
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ underlying OSDs

---

### 7. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒÑŽ

#### ÐžÐ±Ñ‰ÐµÐµ Ð·Ð°Ð¼ÐµÐ´Ð»ÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð°
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ‰ÑƒÑŽ Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ
ceph osd perf
rados bench -p <pool> 10 write --no-cleanup
rados bench -p <pool> 10 seq

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ network
iperf3 Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¾Ð´Ð°Ð¼Ð¸

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ disk I/O
fio --name=test --size=1G --rw=randwrite --bs=4k --direct=1 --filename=/path/to/test
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- Tune kernel parameters:
  ```bash
  echo "net.core.rmem_max = 134217728" >> /etc/sysctl.conf
  echo "net.core.wmem_max = 134217728" >> /etc/sysctl.conf
  sysctl -p
  ```
- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ceph Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹:
  ```bash
  ceph config set global osd_max_backfills 1
  ceph config set global osd_recovery_max_active 3
  ```

---

### 8. ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ ÑÐµÑ‚ÑŒÑŽ

#### High network latency
```bash
# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ latency
ceph osd perf
ceph daemon osd.<id> dump_historic_ops

# ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ network interface
ip -s link
ethtool <interface>
```

**Ð ÐµÑˆÐµÐ½Ð¸Ñ:**
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ MTU settings (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ 9000 Ð´Ð»Ñ jumbo frames)
- ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ network congestion
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ dedicated network Ð´Ð»Ñ Ceph cluster/public networks

---

## Pre-Flight Checklist

### ÐŸÐµÑ€ÐµÐ´ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¾Ð¹ Ceph

- [ ] **Hardware Requirements**
  - [ ] ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 3 Ð½Ð¾Ð´Ñ‹ Ð´Ð»Ñ production
  - [ ] ÐšÐ°Ð¶Ð´Ð°Ñ Ð½Ð¾Ð´Ð° Ð¸Ð¼ÐµÐµÑ‚ Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 16GB RAM
  - [ ] Ð’Ñ‹Ð´ÐµÐ»ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð¸ÑÐºÐ¸ Ð´Ð»Ñ OSD (Ð½Ðµ root Ð´Ð¸ÑÐº)
  - [ ] 10Gbit network (Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ)
  - [ ] ÐžÑ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ network interfaces Ð´Ð»Ñ public/cluster ÑÐµÑ‚Ð¸ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)

- [ ] **Node Preparation**
  - [ ] Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ (NTP) Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð° Ð½Ð° Ð²ÑÐµÑ… Ð½Ð¾Ð´Ð°Ñ…
  - [ ] Ð’ÑÐµ Ð½Ð¾Ð´Ñ‹ Ð¸Ð¼ÐµÑŽÑ‚ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ hostnames
  - [ ] Firewall rules Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹ (Ð¿Ð¾Ñ€Ñ‚Ñ‹ 6789, 3300, 6800-7300)
  - [ ] Kernel modules: rbd, ceph
  - [ ] Ð”Ð¸ÑÐºÐ¸ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ñ‹ Ð¸ Ð½Ðµ Ð¸Ð¼ÐµÑŽÑ‚ partitions/filesystems

- [ ] **Kubernetes Cluster**
  - [ ] Kubernetes Ð²ÐµÑ€ÑÐ¸Ð¸ 1.22+
  - [ ] Helm 3 ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½
  - [ ] kubectl Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
  - [ ] CSI snapshotter CRDs ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹ (Ð´Ð»Ñ snapshots)

- [ ] **Network**
  - [ ] Network connectivity Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÑÐµÐ¼Ð¸ Ð½Ð¾Ð´Ð°Ð¼Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐµÐ½Ð°
  - [ ] DNS resolution Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
  - [ ] MTU Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾ Ð½Ð° Ð²ÑÐµÑ… Ð½Ð¾Ð´Ð°Ñ…
  - [ ] Bandwidth Ñ‚ÐµÑÑ‚ Ð¿Ñ€Ð¾Ð¹Ð´ÐµÐ½ (iperf3)

---

## Post-Installation Checklist

### Ð¡Ñ€Ð°Ð·Ñƒ Ð¿Ð¾ÑÐ»Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸

- [ ] **Cluster Health**
  - [ ] `ceph status` Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ HEALTH_OK
  - [ ] Ð’ÑÐµ OSD Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ up Ð¸ in: `ceph osd tree`
  - [ ] Ð’ÑÐµ MON Ð² quorum: `ceph mon stat`
  - [ ] PGs Ð² Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸: `ceph pg stat`

- [ ] **Storage Configuration**
  - [ ] Pools ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼ replication size
  - [ ] StorageClasses ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚
  - [ ] Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ PVC ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð¸ Ð¼Ð¾Ð½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ÑÑ
  - [ ] RBD Ð¸ CephFS provisioners Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽÑ‚

- [ ] **Security**
  - [ ] RBAC Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾
  - [ ] Secrets ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹ Ð´Ð»Ñ authentication
  - [ ] Network policies Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ñ‹ (ÐµÑÐ»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ÑÑ)

- [ ] **Monitoring**
  - [ ] Prometheus metrics Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹
  - [ ] Grafana dashboards Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹
  - [ ] Alerts Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ñ‹
  - [ ] Log aggregation Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚

- [ ] **Backup**
  - [ ] Backup strategy Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð°
  - [ ] Disaster recovery plan ÑÐ¾Ð·Ð´Ð°Ð½
  - [ ] Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð°

---

## Daily Operations Checklist

### Ð•Ð¶ÐµÐ´Ð½ÐµÐ²Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸

- [ ] **Health Check**
  - [ ] `ceph -s` - Ð¾Ð±Ñ‰Ð¸Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ
  - [ ] `ceph health detail` - Ð´ÐµÑ‚Ð°Ð»Ð¸ warning/errors
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Grafana dashboards

- [ ] **Capacity Planning**
  - [ ] `ceph df` - Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ðµ Ð¼ÐµÑÑ‚Ð¾
  - [ ] `ceph osd df tree` - Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ balance OSD
  - [ ] Ð¡Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð³Ð´Ð° Ð¿Ð¾Ð½Ð°Ð´Ð¾Ð±Ð¸Ñ‚ÑÑ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ

- [ ] **Performance**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ slow ops: `ceph health detail | grep slow`
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ latency: `ceph osd perf`
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ I/O metrics Ð² monitoring

- [ ] **Logs Review**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð² Ð»Ð¾Ð³Ð°Ñ…
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Kubernetes events: `kubectl get events -n rook-ceph`

---

## Weekly Maintenance Checklist

### Ð•Ð¶ÐµÐ½ÐµÐ´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸

- [ ] **Deep Scrub Status**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ scrubbing Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾
  - [ ] `ceph pg dump | grep scrub`
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ errors Ð¿Ð¾ÑÐ»Ðµ scrub

- [ ] **Backup Verification**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ backups ÑÐ¾Ð·Ð´Ð°ÑŽÑ‚ÑÑ
  - [ ] ÐŸÑ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ (Ð¿ÐµÑ€Ð¸Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸)
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ retention policies

- [ ] **Updates Review**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Rook/Ceph
  - [ ] Ð—Ð°Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ maintenance window ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ changelog Ð¸ breaking changes

- [ ] **Capacity Trends**
  - [ ] ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€Ð¾ÑÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð·Ð° Ð½ÐµÐ´ÐµÐ»ÑŽ
  - [ ] ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ capacity planning
  - [ ] Ð—Ð°ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾Ðµ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐµÑÐ»Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾

- [ ] **Documentation**
  - [ ] ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ runbooks Ð¿Ñ€Ð¸ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ð¸ Ð¸Ð½Ñ†Ð¸Ð´ÐµÐ½Ñ‚Ð¾Ð²
  - [ ] Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸
  - [ ] ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ architecture diagrams

---

## Monthly Maintenance Checklist

### Ð•Ð¶ÐµÐ¼ÐµÑÑÑ‡Ð½Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸

- [ ] **Disaster Recovery Test**
  - [ ] ÐŸÑ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ backup restore Ð¿Ñ€Ð¾Ñ†ÐµÐ´ÑƒÑ€Ñƒ
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ RTO/RPO ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ
  - [ ] ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ DR Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ

- [ ] **Performance Tuning**
  - [ ] ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ performance metrics Ð·Ð° Ð¼ÐµÑÑÑ†
  - [ ] ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ placement groups ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
  - [ ] Tune CRUSH map Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸

- [ ] **Security Audit**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ access logs
  - [ ] ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ certificates ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð±Ð»Ð¸Ð¶Ð°ÐµÑ‚ÑÑ expiry
  - [ ] Review Ð¸ rotate secrets

- [ ] **Upgrade Planning**
  - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ roadmap Rook/Ceph
  - [ ] Ð—Ð°Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð°Ð¿Ð³Ñ€ÐµÐ¹Ð´Ñ‹
  - [ ] ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚ÐµÑÑ‚Ð¾Ð²ÑƒÑŽ ÑÑ€ÐµÐ´Ñƒ Ð´Ð»Ñ Ð°Ð¿Ð³Ñ€ÐµÐ¹Ð´Ð¾Ð²

---

## Emergency Response Checklist

### ÐŸÑ€Ð¸ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½Ð¾Ð²ÐµÐ½Ð¸Ð¸ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹

1. **Immediate Assessment** (0-5 Ð¼Ð¸Ð½ÑƒÑ‚)
   - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ `ceph status`
   - [ ] Ð˜Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ scope Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ (OSD/MON/MDS/PG)
   - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÐµÑÑ‚ÑŒ Ð»Ð¸ data loss
   - [ ] ÐžÑ†ÐµÐ½Ð¸Ñ‚ÑŒ impact Ð½Ð° applications

2. **Containment** (5-15 Ð¼Ð¸Ð½ÑƒÑ‚)
   - [ ] Ð˜Ð·Ð¾Ð»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ ÐµÑÐ»Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾
   - [ ] ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ€Ð°ÑÐ¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹
   - [ ] Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ logs Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
   - [ ] Ð£Ð²ÐµÐ´Ð¾Ð¼Ð¸Ñ‚ÑŒ stakeholders

3. **Investigation** (15-30 Ð¼Ð¸Ð½ÑƒÑ‚)
   - [ ] Ð¡Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð»Ð¾Ð³Ð¸
   - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ recent changes
   - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ monitoring/alerting Ð´Ð°Ð½Ð½Ñ‹Ðµ
   - [ ] ÐÐ°Ð¹Ñ‚Ð¸ root cause

4. **Recovery** (30+ Ð¼Ð¸Ð½ÑƒÑ‚)
   - [ ] ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð¸Ð· runbook
   - [ ] Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½ÑƒÑŽ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ
   - [ ] ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ data integrity
   - [ ] ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾ÑÐ»Ðµ recovery

5. **Post-Incident** (Ð¿Ð¾ÑÐ»Ðµ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ)
   - [ ] ÐÐ°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ incident report
   - [ ] ÐŸÑ€Ð¾Ð²ÐµÑÑ‚Ð¸ post-mortem meeting
   - [ ] ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ runbooks/documentation
   - [ ] Ð’Ð½ÐµÐ´Ñ€Ð¸Ñ‚ÑŒ preventive measures
   - [ ] Update monitoring/alerting

---

## Useful Commands Reference

### Quick Diagnostic Commands

```bash
# ÐžÐ±Ñ‰Ð¸Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ
ceph -s
ceph health detail
ceph df

# OSD status
ceph osd tree
ceph osd stat
ceph osd df tree

# MON status
ceph mon stat
ceph quorum_status

# PG status
ceph pg stat
ceph pg dump

# Pool info
ceph osd pool ls detail
ceph osd pool stats

# Performance
ceph osd perf
ceph tell osd.* bench

# Logs
kubectl logs -n rook-ceph deploy/rook-ceph-operator
kubectl logs -n rook-ceph -l app=rook-ceph-mon
kubectl logs -n rook-ceph -l app=rook-ceph-osd

# Events
kubectl get events -n rook-ceph --sort-by='.lastTimestamp'
```

### Configuration Commands

```bash
# Show config
ceph config dump
ceph config show osd.<id>

# Set config
ceph config set osd <parameter> <value>
ceph config set global <parameter> <value>

# Reset config
ceph config rm osd <parameter>
```

---

## Monitoring Key Metrics

### Critical Metrics to Watch

**Cluster Health:**
- Overall health status
- Number of down/out OSDs
- PG states distribution
- MON quorum status

**Capacity:**
- Used capacity percentage (alert at 75%, critical at 85%)
- Available capacity
- Growth rate
- Individual OSD usage

**Performance:**
- IOPS (read/write)
- Throughput (MB/s)
- Latency (commit/apply)
- Client I/O

**Network:**
- Bandwidth utilization
- Packet loss
- Latency between nodes

**Resource Usage:**
- CPU utilization
- Memory usage
- Disk I/O wait
- Network I/O

---

## Contact & Escalation

### Support Channels

1. **Documentation:** 
   - Rook: https://rook.io/docs/
   - Ceph: https://docs.ceph.com/

2. **Community:**
   - Rook Slack: https://rook.io/slack
   - Ceph Mailing Lists: https://ceph.io/community/

3. **Emergency Contacts:**
   - [Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹ Ð²Ð°ÑˆÐµÐ¹ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹]
   - [Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ on-call rotation]
   - [Ð”Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ escalation path]

---

